{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFMTEswPmYTOfputGQcFL5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagardampba2022w/NLP_with_hugging_face/blob/main/nlp_hf_07_question_answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agYttvtFbR56",
        "outputId": "c848e024-c63c-4069-fcd1-7b10871c00ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (1.26.19)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Uncomment and run this cell if you're on Colab or Kaggle\n",
        "!git clone https://github.com/nlp-with-transformers/notebooks.git\n",
        "%cd notebooks\n",
        "from install import *\n",
        "install_requirements(is_chapter7_v2=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "MOP_ZgfELfQu",
        "outputId": "aa441314-c6cc-4aa9-d04e-40883cc4954d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'notebooks'...\n",
            "remote: Enumerating objects: 526, done.\u001b[K\n",
            "remote: Counting objects: 100% (526/526), done.\u001b[K\n",
            "remote: Compressing objects: 100% (289/289), done.\u001b[K\n",
            "remote: Total 526 (delta 251), reused 480 (delta 231), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (526/526), 29.30 MiB | 15.50 MiB/s, done.\n",
            "Resolving deltas: 100% (251/251), done.\n",
            "/content/notebooks\n",
            "‚è≥ Installing base requirements ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "üò≠ Failed to install base requirements",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-b4eb85a7fd1f>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'notebooks'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0minstall\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0minstall_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_chapter7_v2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/notebooks/install.py\u001b[0m in \u001b[0;36minstall_requirements\u001b[0;34m(is_chapter2, is_chapter6, is_chapter7, is_chapter7_v2, is_chapter10, is_chapter11)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprocess_install\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprocess_install\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üò≠ Failed to install base requirements\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Base requirements installed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: üò≠ Failed to install base requirements"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from utils import *\n",
        "setup_chapter()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGyh4LIaLn4o",
        "outputId": "d2d62f01-399d-4a7a-c18b-38c71371ea8d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU was detected! This notebook can be *very* slow without a GPU üê¢\n",
            "Go to Runtime > Change runtime type and select a GPU hardware accelerator.\n",
            "Using transformers v4.39.3\n",
            "Using datasets v2.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env TOKENIZERS_PARALLELISM=false\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsVW-8COLqPJ",
        "outputId": "8d39330c-3f8f-4b51-901d-63b00ede967b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TOKENIZERS_PARALLELISM=false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2RM3Tg5TLiqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Answering"
      ],
      "metadata": {
        "id": "viS4XJFybSYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many flavors of QA, but the most common is extractive QA, which involves\n",
        "questions whose answer can be identified as a span of text in a document, where the\n",
        "document might be a web page, legal contract, or news article. The two-stage process\n",
        "of first retrieving relevant documents and then extracting answers from them is also\n",
        "the basis for many modern QA systems, including semantic search engines, intelli‚Äê\n",
        "gent assistants, and automated information extractors. In this chapter, we‚Äôll apply this\n",
        "process to tackle a common problem facing ecommerce websites: helping consumers\n",
        "answer specific queries to evaluate a product. We‚Äôll see that customer reviews can be\n",
        "used as a rich and challenging source of information for QA, and along the way we‚Äôll\n",
        "learn how transformers act as powerful reading comprehension models that can\n",
        "extract meaning from text. Let‚Äôs begin by fleshing out the use case."
      ],
      "metadata": {
        "id": "eF7dKszubkkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chapter focuses on extractive QA, but other forms of QA may\n",
        "be more suitable for your use case. For example, community QA\n",
        "involves gathering question-answer pairs that are generated by\n",
        "users on forums like Stack Overflow, and then using semantic sim‚Äê\n",
        "ilarity search to find the closest matching answer to a new ques‚Äê\n",
        "tion. There is also long-form QA, which aims to generate complex\n",
        "paragraph-length answers to open-ended questions like ‚ÄúWhy is\n",
        "the sky blue?‚Äù Remarkably, it is also possible to do QA over tables,\n",
        "and transformer models like TAPAS can even perform aggrega‚Äê\n",
        "tions to produce the final answer!"
      ],
      "metadata": {
        "id": "Arp6_udRbqSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Review-Based QA System\n",
        "\n",
        "The Dataset\n",
        "To build our QA system we‚Äôll use the SubjQA dataset,2\n",
        "\n",
        "which consists of more than\n",
        "10,000 customer reviews in English about products and services in six domains: Trip‚Äê\n",
        "Advisor, Restaurants, Movies, Books, Electronics, and Grocery.\n",
        "\n",
        "each review is associated with a question that can be answered using one\n",
        "or more sentences from the review.\n",
        "\n",
        "The interesting aspect of this dataset is that most of the questions and answers are\n",
        "subjective; that is, they depend on the personal experience of the users.\n",
        "\n",
        "\n",
        "First, the query is about ‚Äúpoor quality,‚Äù which is subjective and depends on the\n",
        "user‚Äôs definition of quality. Second, important parts of the query do not appear in the\n",
        "review at all, which means it cannot be answered with shortcuts like keyword search\n",
        "or paraphrasing the input question. These features make SubjQA a realistic dataset to\n",
        "benchmark our review-based QA models on,"
      ],
      "metadata": {
        "id": "kE3AAMi4btM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get started, let‚Äôs download the dataset from the Hugging Face Hub. As we did in\n",
        "Chapter 4, we can use the get_dataset_config_names() function to find out which\n",
        "subsets are available:"
      ],
      "metadata": {
        "id": "mR3Niek-cGxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import get_dataset_config_names\n",
        "domains = get_dataset_config_names(\"subjqa\")\n",
        "domains"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKRKI8Sfb0xt",
        "outputId": "6b20213e-fbe4-47b1-bece-4cefe0e16e6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['books', 'electronics', 'grocery', 'movies', 'restaurants', 'tripadvisor']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "subjqa = load_dataset(\"subjqa\", name=\"electronics\")"
      ],
      "metadata": {
        "id": "3DcI9sqHbk_7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(subjqa[\"train\"][\"answers\"][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FRr0XB6bTZe",
        "outputId": "89d6c224-71fa-4c8d-d869-307b281ff8bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': ['Bass is weak as expected', 'Bass is weak as expected, even with EQ adjusted up'], 'answer_start': [1302, 1302], 'answer_subj_level': [1, 1], 'ans_subj_score': [0.5083333253860474, 0.5083333253860474], 'is_ans_subjective': [True, True]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To explore the dataset more easily, we‚Äôll flatten these nested columns with the flatten() method and convert each split to a Pandas\n",
        "DataFrame as follows:"
      ],
      "metadata": {
        "id": "CU_Hp22Gch9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dfs = {split: dset.to_pandas() for split, dset in subjqa.flatten().items()}\n",
        "\n",
        "\n",
        "for split, df in dfs.items():\n",
        "  print(f\"Number of questions in {split}: {df['id'].nunique()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8oWwZvzcnwf",
        "outputId": "30fbb745-c932-4e91-c320-964c23751e5b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of questions in train: 1295\n",
            "Number of questions in test: 358\n",
            "Number of questions in validation: 255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs focus on these columns and take a look at a few of the training examples. We can\n",
        "use the sample() method to select a random sample:"
      ],
      "metadata": {
        "id": "89rN02Jzc0gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_cols = [\"title\", \"question\", \"answers.text\",\"answers.answer_start\", \"context\"]\n",
        "sample_df = dfs[\"train\"][qa_cols].sample(2, random_state=7)\n",
        "sample_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "RPABbm6xc1BX",
        "outputId": "e5e0817a-64c2-4c2d-a2cd-081fc749a603"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           title                        question                answers.text  \\\n",
              "791   B005DKZTMG  Does the keyboard lightweight?  [this keyboard is compact]   \n",
              "1159  B00AAIPT76             How is the battery?                          []   \n",
              "\n",
              "     answers.answer_start                                            context  \n",
              "791                 [215]  I really like this keyboard.  I give it 4 star...  \n",
              "1159                   []  I bought this after the first spare gopro batt...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d60a7d41-7cf6-4bc5-a83a-5c19262d48ea\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>question</th>\n",
              "      <th>answers.text</th>\n",
              "      <th>answers.answer_start</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>791</th>\n",
              "      <td>B005DKZTMG</td>\n",
              "      <td>Does the keyboard lightweight?</td>\n",
              "      <td>[this keyboard is compact]</td>\n",
              "      <td>[215]</td>\n",
              "      <td>I really like this keyboard.  I give it 4 star...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1159</th>\n",
              "      <td>B00AAIPT76</td>\n",
              "      <td>How is the battery?</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>I bought this after the first spare gopro batt...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d60a7d41-7cf6-4bc5-a83a-5c19262d48ea')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d60a7d41-7cf6-4bc5-a83a-5c19262d48ea button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d60a7d41-7cf6-4bc5-a83a-5c19262d48ea');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e3a4d89b-f75c-4b3e-962c-3840365adf70\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e3a4d89b-f75c-4b3e-962c-3840365adf70')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e3a4d89b-f75c-4b3e-962c-3840365adf70 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_c920310e-114c-46b1-8c3d-53c413eb56e2\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('sample_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c920310e-114c-46b1-8c3d-53c413eb56e2 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('sample_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "sample_df",
              "summary": "{\n  \"name\": \"sample_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"B00AAIPT76\",\n          \"B005DKZTMG\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"How is the battery?\",\n          \"Does the keyboard lightweight?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answers.text\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answers.answer_start\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"I bought this after the first spare gopro battery I bought wouldn't hold a charge. I have very realistic expectations of this sort of product, I am skeptical of amazing stories of charge time and battery life but I do expect the batteries to hold a charge for a couple of weeks at least and for the charger to work like a charger. In this I was not disappointed. I am a river rafter and found that the gopro burns through power in a hurry so this purchase solved that issue. the batteries held a charge, on shorter trips the extra two batteries were enough and on longer trips I could use my friends JOOS Orange to recharge them.I just bought a newtrent xtreme powerpak and expect to be able to charge these with that so I will not run out of power again.\",\n          \"I really like this keyboard.  I give it 4 stars because it doesn't have a CAPS LOCK key so I never know if my caps are on.  But for the price, it really suffices as a wireless keyboard.  I have very large hands and this keyboard is compact, but I have no complaints.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From these examples we can make a few observations. First, the questions are not\n",
        "grammatically correct, which is quite common in the FAQ sections of ecommerce\n",
        "websites. Second, an empty answers.text entry denotes ‚Äúunanswerable‚Äù questions\n",
        "whose answer cannot be found in the review. Finally, we can use the start index and\n",
        "length of the answer span to slice out the span of text in the review that corresponds\n",
        "to the answer:"
      ],
      "metadata": {
        "id": "Tql7ZfhYdVwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_idx = sample_df[\"answers.answer_start\"].iloc[0][0]\n",
        "end_idx = start_idx + len(sample_df[\"answers.text\"].iloc[0][0])\n",
        "sample_df[\"context\"].iloc[0][start_idx:end_idx]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "372UrKRodhnN",
        "outputId": "440a7e1d-763b-4f4e-d2da-5138f1e6e859"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this keyboard is compact'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let‚Äôs get a feel for what types of questions are in the training set by counting the\n",
        "questions that begin with a few common starting words:\n"
      ],
      "metadata": {
        "id": "CVciqFJVd1Qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "counts = {}\n",
        "question_types = [\"What\", \"How\", \"Is\", \"Does\", \"Do\", \"Was\", \"Where\", \"Why\"]\n",
        "\n",
        "for q in question_types:\n",
        "  counts[q] = dfs[\"train\"][\"question\"].str.startswith(q).value_counts()[True]\n",
        "\n",
        "\n",
        "pd.Series(counts).sort_values().plot.barh()\n",
        "plt.title(\"Frequency of Question Types\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "VL14kwp8d2pS",
        "outputId": "23e62eed-1320-4241-b9f0-a68c76d0b809"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that questions beginning with ‚ÄúHow‚Äù, ‚ÄúWhat‚Äù, and ‚ÄúIs‚Äù are the most com‚Äê\n",
        "mon ones, so let‚Äôs have a look at some examples:"
      ],
      "metadata": {
        "id": "3fOHZVfYeDV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for question_type in [\"How\", \"What\", \"Is\"]:\n",
        "  for question in (\n",
        "    dfs[\"train\"][dfs[\"train\"].question.str.startswith(question_type)]\n",
        "    .sample(n=3, random_state=42)['question']):\n",
        "    print(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMgTy2V7eDvi",
        "outputId": "0b1e878f-21a5-4830-f8c3-14351ee2c430"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How is the camera?\n",
            "How do you like the control?\n",
            "How fast is the charger?\n",
            "What is direction?\n",
            "What is the quality of the construction of the bag?\n",
            "What is your impression of the product?\n",
            "Is this how zoom works?\n",
            "Is sound clear?\n",
            "Is it a wireless keyboard?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Answers from Text"
      ],
      "metadata": {
        "id": "VTV1PHyJeM9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing we‚Äôll need for our QA system is to find a way to identify a potential\n",
        "answer as a span of text in a customer review. For example, if a we have a question\n",
        "like ‚ÄúIs it waterproof?‚Äù and the review passage is ‚ÄúThis watch is waterproof at 30m\n",
        "depth‚Äù, then the model should output ‚Äúwaterproof at 30m‚Äù. To do this we‚Äôll need to\n",
        "understand how to:\n",
        "- ‚Ä¢ Frame the supervised learning problem.\n",
        "- ‚Ä¢ Tokenize and encode text for QA tasks.\n",
        "- ‚Ä¢ Deal with long passages that exceed a model‚Äôs maximum context size.\n",
        "Let‚Äôs start by taking a look at how to frame the problem."
      ],
      "metadata": {
        "id": "7O58pu5TeaTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Span classification\n",
        "The most common way to extract answers from text is by framing the problem as a\n",
        "span classification task, where the start and end tokens of an answer span act as the\n",
        "labels that a model needs to predict.\n",
        "\n",
        "\n",
        "Since our training set is relatively small, with only 1,295 examples, a good strategy is\n",
        "to start with a language model that has already been fine-tuned on a large-scale QA\n",
        "dataset like SQuAD. In general, these models have strong reading comprehension\n",
        "capabilities and serve as a good baseline upon which to build a more accurate system.\n",
        "\n",
        "For extractive QA, we can\n",
        "actually start with a fine-tuned model since the structure of the labels remains the\n",
        "same across datasets.\n",
        "\n",
        "You can find a list of extractive QA models by navigating to the Hugging Face Hub\n",
        "and searching for ‚Äúsquad‚Äù on the Models tab\n",
        "\n"
      ],
      "metadata": {
        "id": "XCn-jmoLezkA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, at the time of writing, there are more than 350 QA models to choose\n",
        "from‚Äîso which one should you pick? In general, the answer depends on various fac‚Äê\n",
        "tors like whether your corpus is mono- or multilingual and the constraints of run‚Äê\n",
        "ning the model in a production environment. Table 7-2 lists a few models that\n",
        "provide a good foundation to build on."
      ],
      "metadata": {
        "id": "aoQIe6jdfEAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the purposes of this chapter, we‚Äôll use a fine-tuned MiniLM model since it is fast\n",
        "to train and will allow us to quickly iterate on the techniques that we‚Äôll be exploring.8\n",
        "As usual, the first thing we need is a tokenizer to encode our texts, so let‚Äôs take a look\n",
        "at how this works for QA tasks."
      ],
      "metadata": {
        "id": "gW1UnKvofJAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing text for QA"
      ],
      "metadata": {
        "id": "eM0k3sGUfKnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "model_ckpt = \"deepset/minilm-uncased-squad2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
      ],
      "metadata": {
        "id": "k6h5UeVtfNkb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e369a617-4652-4d21-b65d-0c1342d74b60"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How much music can this hold?\"\n",
        "context = \"\"\"An MP3 is about 1 MB/minute, so about 6000 hours depending on \\\n",
        "file size.\"\"\"\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\")\n"
      ],
      "metadata": {
        "id": "_0gHMquOeXZP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6virWzMB06gy",
        "outputId": "8b37d3c9-9491-4944-d16d-edfc030bf8a1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  2129,  2172,  2189,  2064,  2023,  2907,  1029,   102,  2019,\n",
              "         23378,  2003,  2055,  1015, 16914,  1013,  3371,  1010,  2061,  2055,\n",
              "         25961,  2847,  5834,  2006,  5371,  2946,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hide_input\n",
        "input_df = pd.DataFrame.from_dict(tokenizer(question, context), orient=\"index\")\n",
        "input_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "7cWz_NtgfYZe",
        "outputId": "40b28f63-f025-4214-a803-9d7df6713870"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 0     1     2     3     4     5     6     7    8     9   ...  \\\n",
              "input_ids       101  2129  2172  2189  2064  2023  2907  1029  102  2019  ...   \n",
              "token_type_ids    0     0     0     0     0     0     0     0    0     1  ...   \n",
              "attention_mask    1     1     1     1     1     1     1     1    1     1  ...   \n",
              "\n",
              "                  18    19     20    21    22    23    24    25    26   27  \n",
              "input_ids       2061  2055  25961  2847  5834  2006  5371  2946  1012  102  \n",
              "token_type_ids     1     1      1     1     1     1     1     1     1    1  \n",
              "attention_mask     1     1      1     1     1     1     1     1     1    1  \n",
              "\n",
              "[3 rows x 28 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-548d4bd4-8ec2-4a0e-9715-955284799890\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>input_ids</th>\n",
              "      <td>101</td>\n",
              "      <td>2129</td>\n",
              "      <td>2172</td>\n",
              "      <td>2189</td>\n",
              "      <td>2064</td>\n",
              "      <td>2023</td>\n",
              "      <td>2907</td>\n",
              "      <td>1029</td>\n",
              "      <td>102</td>\n",
              "      <td>2019</td>\n",
              "      <td>...</td>\n",
              "      <td>2061</td>\n",
              "      <td>2055</td>\n",
              "      <td>25961</td>\n",
              "      <td>2847</td>\n",
              "      <td>5834</td>\n",
              "      <td>2006</td>\n",
              "      <td>5371</td>\n",
              "      <td>2946</td>\n",
              "      <td>1012</td>\n",
              "      <td>102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>token_type_ids</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>attention_mask</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows √ó 28 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-548d4bd4-8ec2-4a0e-9715-955284799890')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-548d4bd4-8ec2-4a0e-9715-955284799890 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-548d4bd4-8ec2-4a0e-9715-955284799890');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3120cd98-fdb3-4294-a3d4-e507dd9c5793\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3120cd98-fdb3-4294-a3d4-e507dd9c5793')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3120cd98-fdb3-4294-a3d4-e507dd9c5793 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_7be647e5-5c91-4df2-8afd-cb1a6d282910\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('input_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7be647e5-5c91-4df2-8afd-cb1a6d282910 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('input_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "input_df"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand how the tokenizer formats the inputs for QA tasks, let‚Äôs decode the\n",
        "input_ids tensor:"
      ],
      "metadata": {
        "id": "0zjN8UPgff3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(inputs[\"input_ids\"][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYwO3PzWfcsx",
        "outputId": "82f85d92-a0dd-4e0d-d6f5-9502f2a71125"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] how much music can this hold? [SEP] an mp3 is about 1 mb / minute, so about 6000 hours depending on file size. [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our text is tokenized, we just need to instantiate the model with a QA head\n",
        "and run the inputs through the forward pass:"
      ],
      "metadata": {
        "id": "7gYbEUHyfj4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2Cjo-9xflOm",
        "outputId": "9a20cd32-2490-48e8-f5de-01e465cddc75"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-0.9862, -4.7750, -5.4025, -5.2378, -5.2863, -5.5117, -4.9819, -6.1880,\n",
            "         -0.9862,  0.2596, -0.2144, -1.7136,  3.7806,  4.8561, -1.0546, -3.9097,\n",
            "         -1.7374, -4.5944, -1.4278,  3.9949,  5.0391, -0.2018, -3.0193, -4.8549,\n",
            "         -2.3107, -3.5110, -3.5713, -0.9862]]), end_logits=tensor([[-0.9623, -5.4733, -5.0326, -5.1639, -5.4278, -5.5151, -5.1749, -4.6233,\n",
            "         -0.9623, -3.7855, -0.8715, -3.7745, -3.0161, -1.1780,  0.1758, -2.7365,\n",
            "          4.8934,  0.3046, -3.1761, -3.2762,  0.8937,  5.6606, -0.3623, -4.9554,\n",
            "         -3.2531, -0.0914,  1.6211, -0.9623]]), hidden_states=None, attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see that we get a QuestionAnsweringModelOutput object as the output of\n",
        "the QA head. As illustrated in Figure 7-4, the QA head corresponds to a linear layer\n",
        "that takes the hidden states from the encoder and computes the logits for the start\n",
        "and end spans.10 This means that we treat QA as a form of token classification, similar\n",
        "to what we encountered for named entity recognition in Chapter 4. To convert the\n",
        "outputs into an answer span, we first need to get the logits for the start and end\n",
        "tokens:"
      ],
      "metadata": {
        "id": "gwon5u7rf0ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "start_logits = outputs.start_logits\n",
        "end_logits = outputs.end_logits\n",
        "\n",
        "\n",
        "#If we compare the shapes of these logits to the input IDs:\n",
        "print(f\"Input IDs shape: {inputs.input_ids.size()}\")\n",
        "print(f\"Start logits shape: {start_logits.size()}\")\n",
        "print(f\"End logits shape: {end_logits.size()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylUzPl15f1E9",
        "outputId": "42ddf9c7-8ed0-4ec5-8f5c-43bf35c1f02d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs shape: torch.Size([1, 28])\n",
            "Start logits shape: torch.Size([1, 28])\n",
            "End logits shape: torch.Size([1, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTLQJs3S1OWs",
        "outputId": "bb9f8ea1-4259-4c4d-84a8-7a085b8e127b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.9862, -4.7750, -5.4025, -5.2378, -5.2863, -5.5117, -4.9819, -6.1880,\n",
              "         -0.9862,  0.2596, -0.2144, -1.7136,  3.7806,  4.8561, -1.0546, -3.9097,\n",
              "         -1.7374, -4.5944, -1.4278,  3.9949,  5.0391, -0.2018, -3.0193, -4.8549,\n",
              "         -2.3107, -3.5110, -3.5713, -0.9862]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#id qa-scores\n",
        "#caption Predicted logits for the start and end tokens; the token with the highest score is colored in orange\n",
        "\n",
        "# The idea for this visualisation comes from https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "s_scores = start_logits.detach().numpy().flatten()\n",
        "e_scores = end_logits.detach().numpy().flatten()\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True)\n",
        "colors = [\"C0\" if s != np.max(s_scores) else \"C1\" for s in s_scores]\n",
        "ax1.bar(x=tokens, height=s_scores, color=colors)\n",
        "ax1.set_ylabel(\"Start Scores\")\n",
        "colors = [\"C0\" if s != np.max(e_scores) else \"C1\" for s in e_scores]\n",
        "ax2.bar(x=tokens, height=e_scores, color=colors)\n",
        "ax2.set_ylabel(\"End Scores\")\n",
        "plt.xticks(rotation=\"vertical\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "PQx1oi1YgNWo",
        "outputId": "43711c3e-362d-4425-ec99-2ccc1facf2be"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we see that there are two logits (a start and end) associated with each input token. As\n",
        "illustrated in Figure 7-6, larger, positive logits correspond to more likely candidates\n",
        "for the start and end tokens. In this example we can see that the model assigns the\n",
        "highest start token logits to the numbers ‚Äú1‚Äù and ‚Äú6000‚Äù, which makes sense since our\n",
        "question is asking about a quantity. Similarly, we see that the end tokens with the\n",
        "highest logits are ‚Äúminute‚Äù and ‚Äúhours‚Äù."
      ],
      "metadata": {
        "id": "3Pfmcjsfgbzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the final answer, we can compute the argmax over the start and end token log‚Äê\n",
        "its and then slice the span from the inputs. The following code performs these steps\n",
        "and decodes the result so we can print the resulting text:"
      ],
      "metadata": {
        "id": "R-FbDd-tgtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "start_idx = torch.argmax(start_logits)\n",
        "end_idx = torch.argmax(end_logits) + 1\n",
        "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
        "answer = tokenizer.decode(answer_span)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOP6NmkSge8J",
        "outputId": "f4603f52-7e69-455c-a107-97c25052928e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: How much music can this hold?\n",
            "Answer: 6000 hours\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, it worked! In Transformers, all of these preprocessing and postprocessing\n",
        "steps are conveniently wrapped in a dedicated pipeline. We can instantiate the pipe‚Äê\n",
        "line by passing our tokenizer and fine-tuned model as follows:"
      ],
      "metadata": {
        "id": "S387tXuhgz7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
        "pipe(question=question, context=context, topk=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4eh7CQSg0jT",
        "outputId": "6eb2b178-b548-4fa5-f22a-fd168bd40b03"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/question_answering.py:326: UserWarning: topk parameter is deprecated, use top_k instead\n",
            "  warnings.warn(\"topk parameter is deprecated, use top_k instead\", UserWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.2651614844799042, 'start': 38, 'end': 48, 'answer': '6000 hours'},\n",
              " {'score': 0.22082969546318054,\n",
              "  'start': 16,\n",
              "  'end': 48,\n",
              "  'answer': '1 MB/minute, so about 6000 hours'},\n",
              " {'score': 0.10253532975912094,\n",
              "  'start': 16,\n",
              "  'end': 27,\n",
              "  'answer': '1 MB/minute'}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to the answer, the pipeline also returns the model‚Äôs probability estimate in\n",
        "the score field (obtained by taking a softmax over the logits). This is handy when we\n",
        "want to compare multiple answers within a single context."
      ],
      "metadata": {
        "id": "BHcdPzhZhAgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(question=\"Why is there no data?\", context=context,\n",
        "     handle_impossible_answer=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO0MtMmwhLpo",
        "outputId": "f00487a6-7552-42af-ea19-a940caacf57f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.9068413972854614, 'start': 0, 'end': 0, 'answer': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dealing with long passages\n",
        "\n",
        "One subtlety faced by reading comprehension models is that the context often con‚Äê\n",
        "tains more tokens than the maximum sequence length of the model (which is usually\n",
        "a few hundred tokens at most).\n",
        "\n",
        "a decent portion of the\n",
        "SubjQA training set contains question-context pairs that won‚Äôt fit within MiniLM‚Äôs\n",
        "context size of 512 tokens."
      ],
      "metadata": {
        "id": "DQOJjQ4HhDC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#hide_input\n",
        "#id subjqa-dist\n",
        "#caption Distribution of tokens for each question-context pair in the SubjQA training set\n",
        "def compute_input_length(row):\n",
        "    inputs = tokenizer(row[\"question\"], row[\"context\"])\n",
        "    return len(inputs[\"input_ids\"])\n",
        "\n",
        "dfs[\"train\"][\"n_tokens\"] = dfs[\"train\"].apply(compute_input_length, axis=1)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "dfs[\"train\"][\"n_tokens\"].hist(bins=100, grid=False, ec=\"C0\", ax=ax)\n",
        "plt.xlabel(\"Number of tokens in question-context pair\")\n",
        "ax.axvline(x=512, ymin=0, ymax=1, linestyle=\"--\", color=\"C1\",\n",
        "           label=\"Maximum sequence length\")\n",
        "plt.legend()\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "ii4OIjbmhSa_",
        "outputId": "15fbbf71-cdc3-4cf3-b32e-e052cd38bfe3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For other tasks, like text classification, we simply truncated long texts under the\n",
        "assumption that enough information was contained in the embedding of the [CLS]\n",
        "token to generate accurate predictions. For QA, however, this strategy is problematic\n",
        "because the answer to a question could lie near the end of the context and thus would\n",
        "be removed by truncation."
      ],
      "metadata": {
        "id": "yopaPPbOhhSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the standard way to deal with\n",
        "this is to apply a sliding window across the inputs, where each window contains a pas‚Äê\n",
        "sage of tokens that fit in the model‚Äôs context."
      ],
      "metadata": {
        "id": "QGnqoJ4ZhqS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Transformers, we can set return_overflowing_tokens=True in the tokenizer to\n",
        "enable the sliding window. The size of the sliding window is controlled by the\n",
        "max_seq_length argument, and the size of the stride is controlled by doc_stride."
      ],
      "metadata": {
        "id": "PEQEGLphh4L_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs grab the first example from our training set and define a small window to illus‚Äê\n",
        "trate how this works:"
      ],
      "metadata": {
        "id": "CGaCHFURh5g-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = dfs[\"train\"].iloc[0][[\"question\", \"context\"]]\n",
        "tokenized_example = tokenizer(example[\"question\"], example[\"context\"],\n",
        "return_overflowing_tokens=True, max_length=100,\n",
        "stride=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ke9_-NPh7Hs",
        "outputId": "9a073736-1507-4f28-f917-2aaafeb9b19a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case we now get a list of input_ids, one for each window. Let‚Äôs check the num‚Äê\n",
        "ber of tokens we have in each window:"
      ],
      "metadata": {
        "id": "5S9HggQgh_Mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, window in enumerate(tokenized_example[\"input_ids\"]):\n",
        "  print(f\"Window #{idx} has {len(window)} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPvIvCDfh_o0",
        "outputId": "fbd584b5-23a6-46c3-a4bd-6d6a68045c9d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window #0 has 100 tokens\n",
            "Window #1 has 88 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, we can see where two windows overlap by decoding the inputs:\n",
        "for window in tokenized_example[\"input_ids\"]:\n",
        "    print(f\"{tokenizer.decode(window)} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJv4o0Q8hDzx",
        "outputId": "d9a24606-b09e-4e65-fcf9-add992830614"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] how is the bass? [SEP] i have had koss headphones in the past, pro 4aa and qz - 99. the koss portapro is portable and has great bass response. the work great with my android phone and can be \" rolled up \" to be carried in my motorcycle jacket or computer bag without getting crunched. they are very light and do not feel heavy or bear down on your ears even after listening to music with them on all day. the sound is [SEP] \n",
            "\n",
            "[CLS] how is the bass? [SEP] and do not feel heavy or bear down on your ears even after listening to music with them on all day. the sound is night and day better than any ear - bud could be and are almost as good as the pro 4aa. they are \" open air \" headphones so you cannot match the bass to the sealed types, but it comes close. for $ 32, you cannot go wrong. [SEP] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "let‚Äôs look at the other components we need to build an end-to-end QA pipeline."
      ],
      "metadata": {
        "id": "Zw_cohKDiN4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Haystack to Build a QA Pipeline\n",
        "\n",
        "However, in reality our system‚Äôs users will only provide a question\n",
        "about a product, so we need some way of selecting relevant passages from among all\n",
        "the reviews in our corpus.\n",
        "\n",
        "\n",
        "\n",
        "One way to do this would be to concatenate all the reviews\n",
        "of a given product together and feed them to the model as a single, long context.\n",
        "Although simple, the drawback of this approach is that the context can become\n",
        "extremely long and thereby introduce an unacceptable latency for our users‚Äô queries.\n",
        "\n",
        "\n",
        "To handle this, modern QA systems are typically based on the retriever-reader archi‚Äê\n",
        "tecture, which has two main components:"
      ],
      "metadata": {
        "id": "Sj5HHp5EiQYF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ckev4m052_F"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retriever**\n",
        "\n",
        "Responsible for retrieving relevant documents for a given query. Retrievers are\n",
        "usually categorized as sparse or dense.\n",
        "\n",
        "1. Sparse retrievers use word frequencies to\n",
        "represent each document and query as a sparse vector\n",
        "\n",
        "2. On the other hand, dense retrievers use encoders like transformers to repre‚Äê\n",
        "sent the query and document as contextualized embeddings (which are dense\n",
        "vectors). These embeddings encode semantic meaning, and allow dense retriev‚Äê\n",
        "ers to improve search accuracy by understanding the content of the query.\n",
        "\n",
        "\n",
        "**Reader**\n",
        "\n",
        "Responsible for extracting an answer from the documents provided by the\n",
        "retriever. The reader is usually a reading comprehension model,"
      ],
      "metadata": {
        "id": "_T3O6PmlkCaa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build our QA system, we‚Äôll use the Haystack library developed by deepset, a Ger‚Äê\n",
        "man company focused on NLP. Haystack is based on the retriever-reader architec‚Äê\n",
        "ture, abstracts much of the complexity involved in building these systems, and\n",
        "integrates tightly with Transformers."
      ],
      "metadata": {
        "id": "nityTZhckniS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "there are two more components involved\n",
        "when building a QA pipeline with Haystack:\n",
        "1. Document store\n",
        "\n",
        "A document-oriented database that stores documents and metadata which are\n",
        "provided to the retriever at query time\n",
        "2. Pipeline\n",
        "\n",
        "Combines all the components of a QA system to enable custom query flows,\n",
        "merging documents from multiple retrievers, and more"
      ],
      "metadata": {
        "id": "KD_3fdehkroG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To initialize the document store, we first need to download and install Elasticsearch.\n",
        "By following Elasticsearch‚Äôs guide,\n",
        "\n",
        "12 we can grab the latest release for Linux with wget\n",
        "\n",
        "and unpack it with the tar shell command:"
      ],
      "metadata": {
        "id": "LDRHUI0clUMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"\"\"https://artifacts.elastic.co/downloads/elasticsearch/\\\n",
        "elasticsearch-7.9.2-linux-x86_64.tar.gz\"\"\"\n",
        "!wget -nc -q {url}\n",
        "!tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz"
      ],
      "metadata": {
        "id": "BqfgVtKglUnE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to start the Elasticsearch server. Since we‚Äôre running all the code in this\n",
        "book within Jupyter notebooks, we‚Äôll need to use Python‚Äôs Popen() function to spawn a new process. While we‚Äôre at it, let‚Äôs also run the subprocess in the background using\n",
        "the chown shell command:"
      ],
      "metadata": {
        "id": "tYzshp4Rlcmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "# Run Elasticsearch as a background process\n",
        "!chown -R daemon:daemon elasticsearch-7.9.2\n",
        "es_server = Popen(args=['elasticsearch-7.9.2/bin/elasticsearch'],\n",
        "stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1))\n",
        "# Wait until Elasticsearch has started\n",
        "!sleep 30"
      ],
      "metadata": {
        "id": "L0B1xI1UlgPW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, Elasticsearch runs locally on port 9200, so we can test\n",
        "the connection by sending an HTTP request to localhost:"
      ],
      "metadata": {
        "id": "M-Kfe_Rxloa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X GET \"localhost:9200/\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8TH3IIylqAL",
        "outputId": "b364ac35-e9d1-468c-c629-a33cb39db106"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "curl: (7) Failed to connect to localhost port 9200 after 2 ms: Connection refused\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/elasticsearch-7.9.2/config/elasticsearch.yml | grep \"http.port\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7x6_lOAHjE5",
        "outputId": "ac7e20d3-85fc-4b1a-df58-5d85f3dff0de"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#http.port: 9200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "!netstat -tuln | grep LISTEN\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "dhIfnfwBHajC",
        "outputId": "dedfa307-6113-4de0-8d13-6b6e96e27510"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "bash: line 1: !netstat: command not found\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "Command 'b'!netstat -tuln | grep LISTEN\\n'' returned non-zero exit status 1.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-a01f0e177931>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'!netstat -tuln | grep LISTEN\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'!netstat -tuln | grep LISTEN\\n'' returned non-zero exit status 1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "ps -ef | grep elasticsearch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3QYhyUcHL-t",
        "outputId": "177903bb-9f41-45fe-fb83-eea09be20747"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "daemon     23350   22367 88 11:46 ?        00:00:27 /content/elasticsearch-7.9.2/jdk/bin/java -Xshar\n",
            "daemon     23585   23350  0 11:46 ?        00:00:00 /content/elasticsearch-7.9.2/modules/x-pack-ml/p\n",
            "root       23684   23682  0 11:46 ?        00:00:00 grep elasticsearch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our Elasticsearch server is up and running, the next thing to do is instanti‚Äê\n",
        "ate the document store:"
      ],
      "metadata": {
        "id": "KHnY7ZZ7lxe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install haystack-ai\n",
        "#!pip install elasticsearch-haystack\n",
        "#!pip install farm-haystack\n",
        "#!pip show elasticsearch\n",
        "#!pip install elasticsearch-haystack\n",
        "!pip install farm-haystack[elasticsearch]\n"
      ],
      "metadata": {
        "id": "XSq1oQYbl1Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au8vlsanCLp2",
        "outputId": "9bf75662-1d72-4a0a-b74a-752ec8bb8cd4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "daemon     23350   22367 88 11:46 ?        00:00:27 /content/elasticsearch-7.9.2/jdk/bin/java -Xshar\n",
            "daemon     23585   23350  0 11:46 ?        00:00:00 /content/elasticsearch-7.9.2/modules/x-pack-ml/p\n",
            "root       23681   23679  0 11:46 ?        00:00:00 grep elasticsearch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "# Connect to the running Elasticsearch 8.11.1 instance\n",
        "es = Elasticsearch(\n",
        "    hosts=[\"http://localhost:9200\"],\n",
        "    verify_certs=False\n",
        ")\n",
        "\n",
        "# Test the connection\n",
        "if es.ping():\n",
        "    print(\"Elasticsearch 8.11.1 is running!\")\n",
        "else:\n",
        "    print(\"Could not connect to Elasticsearch 8.11.1.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2H23ktkEbTl",
        "outputId": "20a18656-e159-4b74-829a-a97aa30d4a1b"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(http://localhost:9200)> has failed for 1 times in a row, putting on 1 second timeout\n",
            "WARNING:elastic_transport.transport:Retrying request after failure (attempt 0 of 3)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/elastic_transport/_transport.py\", line 342, in perform_request\n",
            "    resp = node.perform_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/elastic_transport/_node/_http_urllib3.py\", line 202, in perform_request\n",
            "    raise err from None\n",
            "elastic_transport.ConnectionError: Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x7837c84b2260>: Failed to establish a new connection: [Errno 111] Connection refused)\n",
            "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(http://localhost:9200)> has failed for 2 times in a row, putting on 2 second timeout\n",
            "WARNING:elastic_transport.transport:Retrying request after failure (attempt 1 of 3)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/elastic_transport/_transport.py\", line 342, in perform_request\n",
            "    resp = node.perform_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/elastic_transport/_node/_http_urllib3.py\", line 202, in perform_request\n",
            "    raise err from None\n",
            "elastic_transport.ConnectionError: Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x7837c84b24a0>: Failed to establish a new connection: [Errno 111] Connection refused)\n",
            "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(http://localhost:9200)> has failed for 3 times in a row, putting on 4 second timeout\n",
            "WARNING:elastic_transport.transport:Retrying request after failure (attempt 2 of 3)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/elastic_transport/_transport.py\", line 342, in perform_request\n",
            "    resp = node.perform_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/elastic_transport/_node/_http_urllib3.py\", line 202, in perform_request\n",
            "    raise err from None\n",
            "elastic_transport.ConnectionError: Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x7837c84b27a0>: Failed to establish a new connection: [Errno 111] Connection refused)\n",
            "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(http://localhost:9200)> has failed for 4 times in a row, putting on 8 second timeout\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not connect to Elasticsearch 8.11.1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TlQDC04pEfVq"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "curl -sX GET \"localhost:9200/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "_VJRxWHFCOsP",
        "outputId": "b1a24484-633c-4d0a-da4c-a75a1371244d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "Command 'b'\\ncurl -sX GET \"localhost:9200/\"\\n'' returned non-zero exit status 7.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-506315f5439d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\ncurl -sX GET \"localhost:9200/\"\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\ncurl -sX GET \"localhost:9200/\"\\n'' returned non-zero exit status 7."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IOqZ5mEUCIsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.document_stores.elasticsearch import ElasticsearchDocumentStore\n",
        "#from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore\n",
        "from haystack import Document\n",
        "# Return the document embedding for later use with dense retriever\n",
        "#document_store = ElasticsearchDocumentStore()\n",
        "document_store = ElasticsearchDocumentStore(return_embedding=True)\n"
      ],
      "metadata": {
        "id": "IZpr-R1ily8P"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nt3Ibjnj9ew3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It's a good idea to flush Elasticsearch with each notebook restart\n",
        "if len(document_store.get_all_documents()) or len(document_store.get_all_labels()) > 0:\n",
        "    document_store.delete_documents(index=\"document\")\n",
        "    document_store.delete_documents(index=\"label\")"
      ],
      "metadata": {
        "id": "6tgDtv1r7aaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, ElasticsearchDocumentStore creates two indices on Elasticsearch: one\n",
        "called document for (you guessed it) storing documents, and another called label for\n",
        "storing the annotated answer spans. For now, we‚Äôll just populate the document index with the SubjQA reviews, and Haystack‚Äôs document stores expect a list of dictionaries\n",
        "with text and meta keys\n",
        "\n",
        "\n",
        "The fields in meta can be used for applying filters during retrieval. For our purposes\n",
        "we‚Äôll include the item_id and q_review_id columns of SubjQA so we can filter by\n",
        "product and question ID, along with the corresponding training split. We can then\n",
        "loop through the examples in each DataFrame and add them to the index with the\n",
        "write_documents() method as follows:"
      ],
      "metadata": {
        "id": "F0p2M2p1JAVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for split, df in dfs.items():\n",
        "    # Exclude duplicate reviews\n",
        "    docs = [{\"content\": row[\"context\"], \"id\": row[\"review_id\"],\n",
        "             \"meta\":{\"item_id\": row[\"title\"], \"question_id\": row[\"id\"],\n",
        "                     \"split\": split}}\n",
        "        for _,row in df.drop_duplicates(subset=\"context\").iterrows()]\n",
        "    document_store.write_documents(docs)\n",
        "\n",
        "print(f\"Loaded {document_store.get_document_count()} documents\")"
      ],
      "metadata": {
        "id": "9cBBtwk9w6Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ta-8zWLLkVM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "pip install --upgrade pip\n",
        "pip install farm-haystack[colab,inference]"
      ],
      "metadata": {
        "id": "sqy0pq6RiOTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96d300b7-3270-4107-e120-5e26e51880c4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Using cached pip-24.2-py3-none-any.whl (1.8 MB)\n",
            "Installing collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-24.2\n",
            "Requirement already satisfied: farm-haystack[colab,inference] in /usr/local/lib/python3.10/dist-packages (1.26.2)\n",
            "Requirement already satisfied: boilerpy3 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (1.0.7)\n",
            "Requirement already satisfied: events in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (0.5)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (0.27.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (4.23.0)\n",
            "Requirement already satisfied: lazy-imports==0.3.1 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (0.3.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (10.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (2.1.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (9.4.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (4.2.2)\n",
            "Requirement already satisfied: posthog in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (3.5.0)\n",
            "Requirement already satisfied: prompthub-py==4.0.0 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (4.0.0)\n",
            "Requirement already satisfied: pydantic<2 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (1.10.17)\n",
            "Requirement already satisfied: quantulum3 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (0.9.2)\n",
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (0.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (2.32.3)\n",
            "Requirement already satisfied: requests-cache<1.0.0 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (0.9.8)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (1.3.2)\n",
            "Requirement already satisfied: sseclient-py in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (1.8.0)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (0.7.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (4.66.5)\n",
            "Requirement already satisfied: transformers==4.39.3 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (4.39.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,inference]) (0.23.5)\n",
            "Collecting sentence-transformers>=2.2.0 (from farm-haystack[colab,inference])\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pillow (from farm-haystack[colab,inference])\n",
            "  Downloading Pillow-9.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from prompthub-py==4.0.0->farm-haystack[colab,inference]) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.3->farm-haystack[colab,inference]) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.3->farm-haystack[colab,inference]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.3->farm-haystack[colab,inference]) (24.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.3->farm-haystack[colab,inference]) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.3->farm-haystack[colab,inference]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.3->farm-haystack[colab,inference]) (0.4.4)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference]) (3.20.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference]) (2.3.1+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference]) (0.32.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.5.0->farm-haystack[colab,inference]) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.5.0->farm-haystack[colab,inference]) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[colab,inference]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[colab,inference]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[colab,inference]) (1.26.19)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[colab,inference]) (2024.7.4)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from requests-cache<1.0.0->farm-haystack[colab,inference]) (1.4.4)\n",
            "Requirement already satisfied: attrs>=21.2 in /usr/local/lib/python3.10/dist-packages (from requests-cache<1.0.0->farm-haystack[colab,inference]) (24.2.0)\n",
            "Requirement already satisfied: cattrs>=22.2 in /usr/local/lib/python3.10/dist-packages (from requests-cache<1.0.0->farm-haystack[colab,inference]) (23.2.3)\n",
            "Requirement already satisfied: url-normalize>=1.4 in /usr/local/lib/python3.10/dist-packages (from requests-cache<1.0.0->farm-haystack[colab,inference]) (1.4.3)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->farm-haystack[colab,inference]) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->farm-haystack[colab,inference]) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->farm-haystack[colab,inference]) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->farm-haystack[colab,inference]) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->farm-haystack[colab,inference]) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->farm-haystack[colab,inference]) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->farm-haystack[colab,inference]) (0.14.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack[colab,inference]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack[colab,inference]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack[colab,inference]) (0.20.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->farm-haystack[colab,inference]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->farm-haystack[colab,inference]) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->farm-haystack[colab,inference]) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog->farm-haystack[colab,inference]) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog->farm-haystack[colab,inference]) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog->farm-haystack[colab,inference]) (2.2.1)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from quantulum3->farm-haystack[colab,inference]) (7.3.1)\n",
            "Requirement already satisfied: num2words in /usr/local/lib/python3.10/dist-packages (from quantulum3->farm-haystack[colab,inference]) (0.5.13)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference]) (5.9.5)\n",
            "Requirement already satisfied: exceptiongroup>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from cattrs>=22.2->requests-cache<1.0.0->farm-haystack[colab,inference]) (1.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference]) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference]) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference]) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference])\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from inflect->quantulum3->farm-haystack[colab,inference]) (4.3.0)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words->quantulum3->farm-haystack[colab,inference]) (0.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,inference]) (1.3.0)\n",
            "Downloading Pillow-9.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4.3/4.3 MB 76.3 MB/s eta 0:00:00\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 pillow-9.0.0 sentence-transformers-3.0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scikit-image 0.23.2 requires pillow>=9.1, but you have pillow 9.0.0 which is incompatible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing a retriever\n",
        "\n",
        "The Elasticsearch document store can be paired with any of the Haystack retrievers,\n",
        "so let‚Äôs start by using a sparse retriever based on BM25 (short for ‚ÄúBest Match 25‚Äù).\n",
        "BM25 is an improved version of the classic Term Frequency-Inverse Document Fre‚Äê\n",
        "quency (TF-IDF) algorithm and represents the question and context as sparse vectors\n",
        "that can be searched efficiently on Elasticsearch."
      ],
      "metadata": {
        "id": "h73HI564JWi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BM25 score measures how\n",
        "much matched text is about a search query and improves on TF-IDF by saturating TF\n",
        "values quickly and normalizing the document length so that short documents are\n",
        "favored over long ones.13"
      ],
      "metadata": {
        "id": "lJj0H3FMJb0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Haystack, the BM25 retriever is used by default in ElasticsearchRetriever, so\n",
        "let‚Äôs initialize this class by specifying the document store we wish to search over:"
      ],
      "metadata": {
        "id": "9XnRHXIjJi7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.retriever.sparse import ElasticsearchRetriever\n",
        "es_retriever = ElasticsearchRetriever(document_store=document_store)"
      ],
      "metadata": {
        "id": "u_EMpHkSJlC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let‚Äôs look at a simple query for a single electronics product in the training set.\n",
        "For review-based QA systems like ours, it‚Äôs important to restrict the queries to a single\n",
        "item because otherwise the retriever would source reviews about products that are\n",
        "not related to a user‚Äôs query. For example, asking ‚ÄúIs the camera quality any good?‚Äù\n",
        "without a product filter could return reviews about phones, when the user might be\n",
        "asking about a specific laptop camera instead."
      ],
      "metadata": {
        "id": "uuG-7HxqJu0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By themselves, the ASIN values in our\n",
        "dataset are a bit cryptic, but we can decipher them with online tools like amazon\n",
        "ASIN or by simply appending the value of item_id to the www.amazon.com/dp/ URL.\n",
        "The following item ID corresponds to one of Amazon‚Äôs Fire tablets, so let‚Äôs use the\n",
        "retriever‚Äôs retrieve() method to ask if it‚Äôs any good for reading with:"
      ],
      "metadata": {
        "id": "JIOyvJW3J1XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "item_id = \"B0074BW614\"\n",
        "query = \"Is it good for reading?\"\n",
        "retrieved_docs = es_retriever.retrieve(\n",
        "query=query, top_k=3, filters={\"item_id\":[item_id], \"split\":[\"train\"]})"
      ],
      "metadata": {
        "id": "W8S2iHHCJ24g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we‚Äôve specified how many documents to return with the top_k argument and\n",
        "applied a filter on both the item_id and split keys that were included in the meta\n",
        "field of our documents. Each element of retrieved_docs is a Haystack Document\n",
        "object that is used to represent documents and includes the retriever‚Äôs query score\n",
        "along with other metadata. Let‚Äôs have a look at one of the retrieved documents:"
      ],
      "metadata": {
        "id": "YEKVYnTzJ6kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(retrieved_docs[0])"
      ],
      "metadata": {
        "id": "0Ac5-_x_KEbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to the document‚Äôs text, we can see the score that Elasticsearch computed\n",
        "for its relevance to the query (larger scores imply a better match). Under the hood,\n",
        "Elasticsearch relies on Lucene for indexing and search, so by default it uses Lucene‚Äôs\n",
        "practical scoring function. in brief terms it first filters the can‚Äê\n",
        "didate documents by applying a Boolean test (does the document match the query?),and then applies a similarity metric that‚Äôs based on representing both the document\n",
        "and the query as vectors."
      ],
      "metadata": {
        "id": "QXjwft2DKE1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a way to retrieve relevant documents, the next thing we need is a\n",
        "way to extract answers from them. This is where the reader comes in, so let‚Äôs take a\n",
        "look at how we can load our MiniLM model in Haystack."
      ],
      "metadata": {
        "id": "zRGwHCijKj3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing a reader"
      ],
      "metadata": {
        "id": "LqOfaMieKm_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FARMReader\n",
        "Based on deepset‚Äôs FARM framework for fine-tuning and deploying transform‚Äê\n",
        "ers. Compatible with models trained using Transformers and can load models\n",
        "directly from the Hugging Face Hub.\n",
        "\n",
        "### TransformersReader\n",
        "Based on the QA pipeline from Transformers. Suitable for running inference\n",
        "only."
      ],
      "metadata": {
        "id": "xCc3eJAZKq6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we will be fine-tuning the reader later in the chapter, we‚Äôll use the FARMReader.\n",
        "As with Transformers, to load the model we just need to specify the MiniLM\n",
        "checkpoint on the Hugging Face Hub along with some QA-specific arguments:"
      ],
      "metadata": {
        "id": "Y3pKm6A6K18C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.nodes import FARMReader\n",
        "\n",
        "model_ckpt = \"deepset/minilm-uncased-squad2\" #alternative larger models: deepset/roberta-base-squad2-distilled or deepset/xlm-roberta-large-squad2 or the tiny distilled model: deepset/tinyroberta-squad2\n",
        "max_seq_length, doc_stride = 384, 128\n",
        "reader = FARMReader(model_name_or_path=model_ckpt, progress_bar=False,\n",
        "                    max_seq_len=max_seq_length, doc_stride=doc_stride,\n",
        "                    return_no_answer=True)"
      ],
      "metadata": {
        "id": "X9GAklg8K9uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reader.predict_on_texts(question=question, texts=[context], top_k=1))\n"
      ],
      "metadata": {
        "id": "b09SfjzSK_XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting it all together\n"
      ],
      "metadata": {
        "id": "KoiVL_I7LBxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.pipelines import ExtractiveQAPipeline\n",
        "\n",
        "pipe = ExtractiveQAPipeline(reader=reader, retriever=bm25_retriever)"
      ],
      "metadata": {
        "id": "MxWfjNWoLCno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you can specify the name of the node, e.g., \"Retriever\", that you want to pass parameters to.\n",
        "\n"
      ],
      "metadata": {
        "id": "GuX4bYC0LI8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_answers = 3\n",
        "preds = pipe.run(query=query, params={\"Retriever\": {\"top_k\": 3, \"filters\":{\"item_id\": [item_id], \"split\":[\"train\"]}},\n",
        "                                      \"Reader\": {\"top_k\": n_answers}})\n",
        "\n",
        "print(f\"Question: {preds['query']} \\n\")\n",
        "\n",
        "for idx in range(n_answers):\n",
        "    print(f\"Answer {idx+1}: {preds['answers'][idx].answer}\")\n",
        "    print(f\"Review snippet: ...{preds['answers'][idx].context}...\")\n",
        "    print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "fVkYocSLKp_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, we now have an end-to-end QA system for Amazon product reviews! This is a\n",
        "good start, but notice that the second and third answers are closer to what the ques‚Äê\n",
        "tion is actually asking. To do better, we‚Äôll need some metrics to quantify the perfor‚Äê\n",
        "mance of the retriever and reader. We‚Äôll take a look at that next."
      ],
      "metadata": {
        "id": "XPHCCdngNn4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improving Our QA Pipeline"
      ],
      "metadata": {
        "id": "iYuqiEHRM3Kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In particular, the\n",
        "retriever sets an upper bound on the performance of the whole QA system, so it‚Äôs\n",
        "important to make sure it‚Äôs doing a good job. With this in mind, let‚Äôs start by intro‚Äê\n",
        "ducing some common metrics to evaluate the retriever so that we can compare the\n",
        "performance of sparse and dense representations."
      ],
      "metadata": {
        "id": "4oBb9q8UNwWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the Retriever\n",
        "A common metric for evaluating retrievers is recall, which measures the fraction of all\n",
        "relevant documents that are retrieved. In this context, ‚Äúrelevant‚Äù simply means\n",
        "whether the answer is present in a passage of text or not, so given a set of questions,\n",
        "we can compute recall by counting the number of times an answer appears in the top\n",
        "k documents returned by the retriever."
      ],
      "metadata": {
        "id": "QAOXSmBFN3Jj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Haystack, there are two ways to evaluate retrievers:\n",
        "‚Ä¢ Use the retriever‚Äôs in-built eval() method. This can be used for both open- and\n",
        "closed-domain QA, but not for datasets like SubjQA where each document is\n",
        "paired with a single product and we need to filter by product ID for every query.\n",
        "‚Ä¢ Build a custom Pipeline that combines a retriever with the EvalRetriever class.\n",
        "This enables the implementation of custom metrics and query flows."
      ],
      "metadata": {
        "id": "nKRfeOpfN6ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from haystack.pipelines import DocumentSearchPipeline\n",
        "\n",
        "pipe = DocumentSearchPipeline(retriever=bm25_retriever)"
      ],
      "metadata": {
        "id": "SsU68SXuM5UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import Label, Answer, Document\n",
        "\n",
        "labels = []\n",
        "for i, row in dfs[\"test\"].iterrows():\n",
        "    # Metadata used for filtering in the Retriever\n",
        "    meta = {\"item_id\": row[\"title\"], \"question_id\": row[\"id\"]}\n",
        "    # Populate labels for questions with answers\n",
        "    if len(row[\"answers.text\"]):\n",
        "        for answer in row[\"answers.text\"]:\n",
        "            label = Label(\n",
        "                query=row[\"question\"], answer=Answer(answer=answer), origin=\"gold-label\", document=Document(content=row[\"context\"], id=row[\"review_id\"]),\n",
        "                meta=meta, is_correct_answer=True, is_correct_document=True,\n",
        "                no_answer=False, filters={\"item_id\": [meta[\"item_id\"]], \"split\":[\"test\"]})\n",
        "            labels.append(label)\n",
        "    # Populate labels for questions without answers\n",
        "    else:\n",
        "        label = Label(\n",
        "            query=row[\"question\"], answer=Answer(answer=\"\"), origin=\"gold-label\", document=Document(content=row[\"context\"], id=row[\"review_id\"]),\n",
        "            meta=meta, is_correct_answer=True, is_correct_document=True,\n",
        "            no_answer=True, filters={\"item_id\": [row[\"title\"]], \"split\":[\"test\"]})\n",
        "        labels.append(label)"
      ],
      "metadata": {
        "id": "G6LUDnVfM-P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_store.write_labels(labels, index=\"label\")\n",
        "\n",
        "print(f\"\"\"Loaded {document_store.get_label_count(index=\"label\")} \\\n",
        "question-answer pairs\"\"\")"
      ],
      "metadata": {
        "id": "cz2wauMyM_pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "labels_agg = document_store.get_all_labels_aggregated(\n",
        "    index=\"label\",\n",
        "    open_domain=True,\n",
        "    aggregate_by_meta=[\"item_id\"]\n",
        ")\n",
        "print(len(labels_agg))"
      ],
      "metadata": {
        "id": "2zui74qANBCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# We can run the pipeline with the desired top_k value like this\n",
        "eval_result = pipe.eval(\n",
        "    labels=labels_agg,\n",
        "    params={\"Retriever\": {\"top_k\": 3}},\n",
        ")\n",
        "metrics = eval_result.calculate_metrics()"
      ],
      "metadata": {
        "id": "SQeUQL-yNET3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Recall@3: {metrics['Retriever']['recall_single_hit']:.2f}\")\n"
      ],
      "metadata": {
        "id": "06JC_W0kNFlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_df = eval_result[\"Retriever\"]\n",
        "eval_df[eval_df[\"query\"] == \"How do you like the lens?\"][[\"query\", \"filters\", \"rank\", \"content\", \"gold_document_contents\", \"document_id\", \"gold_document_ids\", \"gold_id_match\"]]\n",
        ""
      ],
      "metadata": {
        "id": "yJlF-kT6NQtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or if we want to calculate metrics for multiple topk values, we can run the pipeline only once with the highest top_k value and calculate metrics for smaller top_ks afterwards.\n",
        "\n"
      ],
      "metadata": {
        "id": "9Ehd0wxuNV0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_retriever(retriever, topk_values = [1,3,5,10,20]):\n",
        "    topk_results = {}\n",
        "    # Calculate max top_k\n",
        "    max_top_k = max(topk_values)\n",
        "    # Create Pipeline\n",
        "    p = DocumentSearchPipeline(retriever=retriever)\n",
        "    # Run inference with max top_k by looping over each question-answers pair in test set\n",
        "    eval_result = p.eval(\n",
        "        labels=labels_agg,\n",
        "        params={\"Retriever\": {\"top_k\": max_top_k}},\n",
        "    )\n",
        "    # Calculate metric for each top_k value\n",
        "    for topk in topk_values:\n",
        "        # Get metrics\n",
        "        metrics = eval_result.calculate_metrics(simulated_top_k_retriever=topk)\n",
        "        topk_results[topk] = {\"recall\": metrics[\"Retriever\"][\"recall_single_hit\"]}\n",
        "\n",
        "    return pd.DataFrame.from_dict(topk_results, orient=\"index\")\n",
        "\n",
        "\n",
        "bm25_topk_df = evaluate_retriever(bm25_retriever)"
      ],
      "metadata": {
        "id": "S-yu9mnQNX3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_retriever_eval(dfs, retriever_names):\n",
        "    fig, ax = plt.subplots()\n",
        "    for df, retriever_name in zip(dfs, retriever_names):\n",
        "        df.plot(y=\"recall\", ax=ax, label=retriever_name)\n",
        "    plt.xticks(df.index)\n",
        "    plt.ylabel(\"Top-k Recall\")\n",
        "    plt.xlabel(\"k\")\n",
        "    plt.show()\n",
        "\n",
        "plot_retriever_eval([bm25_topk_df], [\"BM25\"])"
      ],
      "metadata": {
        "id": "LMUka5K5NZOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the plot, we can see that there‚Äôs an inflection point around k = 5 and we get\n",
        "almost perfect recall from k = 10 onwards. Let‚Äôs now take a look at retrieving docu‚Äê\n",
        "ments with dense vector techniques."
      ],
      "metadata": {
        "id": "WyioF7UeOkGO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IQIgPUOOKIGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CvNigcRmhBLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dense Passage Retrieval\n",
        "\n",
        "We‚Äôve seen that we get almost perfect recall when our sparse retriever returns k = 10\n",
        "documents, but can we do better at smaller values of k? The advantage of doing so is\n",
        "that we can pass fewer documents to the reader and thereby reduce the overall\n",
        "latency of our QA pipeline. A well-known limitation of sparse retrievers like BM25 is\n",
        "that they can fail to capture the relevant documents if the user query contains terms\n",
        "that don‚Äôt match exactly those of the review. One promising alternative is to use dense\n",
        "embeddings to represent the question and document, and the current state of the art\n",
        "is an architecture known as Dense Passage Retrieval (DPR).14"
      ],
      "metadata": {
        "id": "gmYYk-3jgP-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main idea behind\n",
        "DPR is to use two BERT models as encoders for the question and the passage. As\n",
        "illustrated in Figure 7-10, these encoders map the input text into a d-dimensional\n",
        "vector representation of the [CLS] token."
      ],
      "metadata": {
        "id": "MSUGYC0COv_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Haystack, we can initialize a retriever for DPR in a similar way to what we did for\n",
        "BM25. In addition to specifying the document store, we also need to pick the BERT\n",
        "encoders for the question and passage. These encoders are trained by giving them\n",
        "questions with relevant (positive) passages and irrelevant (negative) passages, where\n",
        "the goal is to learn that relevant question-passage pairs have a higher similarity. For\n",
        "our use case, we‚Äôll use encoders that have been fine-tuned on the NQ corpus in this\n",
        "way:"
      ],
      "metadata": {
        "id": "cXcMWYWoOyTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.retriever.dense import DensePassageRetriever\n",
        "dpr_retriever = DensePassageRetriever(document_store=document_store,\n",
        "query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
        "passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
        "embed_title=False)"
      ],
      "metadata": {
        "id": "5fC8a3zcO4R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we‚Äôve initialized the dense retriever, the next step is to iterate over all the\n",
        "indexed documents in our Elasticsearch index and apply the encoders to update the\n",
        "embedding representation. This can be done as follows:"
      ],
      "metadata": {
        "id": "7S-aSSE7PAYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_store.update_embeddings(retriever=dpr_retriever)"
      ],
      "metadata": {
        "id": "6YDJqmahPAur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We‚Äôre now set to go! We can evaluate the dense retriever in the same way we did for\n",
        "BM25 and compare the top-k recall:"
      ],
      "metadata": {
        "id": "3Zj-9yrpPDnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dpr_topk_df = evaluate_retriever(dpr_retriever)\n",
        "plot_retriever_eval([es_topk_df, dpr_topk_df], [\"BM25\", \"DPR\"])"
      ],
      "metadata": {
        "id": "Neb9DrvZPFbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see that DPR does not provide a boost in recall over BM25 and saturates\n",
        "around k = 3."
      ],
      "metadata": {
        "id": "RmcSnrhJPInL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the Reader\n",
        "In extractive QA, there are two main metrics that are used for evaluating readers:\n",
        "\n",
        "1. Exact Match (EM)\n",
        "\n",
        "A binary metric that gives EM = 1 if the characters in the predicted and ground\n",
        "truth answers match exactly, and EM = 0 otherwise. If no answer is expected, the\n",
        "model gets EM = 0 if it predicts any text at all.\n",
        "\n",
        "2. F1 -score\n",
        "\n",
        "Measures the harmonic mean of the precision and recall."
      ],
      "metadata": {
        "id": "lfvnqV8BPMxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from farm.evaluation.squad_evaluation import compute_f1, compute_exact\n",
        "pred = \"about 6000 hours\"\n",
        "label = \"6000 hours\"\n",
        "print(f\"EM: {compute_exact(label, pred)}\")\n",
        "print(f\"F1: {compute_f1(label, pred)}\")\n",
        "EM: 0\n",
        "F1: 0.8"
      ],
      "metadata": {
        "id": "43ibhQrgPXvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Under the hood, these functions first normalize the prediction and label by removing\n",
        "punctuation, fixing whitespace, and converting to lowercase. The normalized strings\n",
        "are then tokenized as a bag-of-words, before finally computing the metric at the\n",
        "token level. From this simple example we can see that EM is a much stricter metric\n",
        "than the F1\n",
        "-score:"
      ],
      "metadata": {
        "id": "XFArWMHcPbz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "adding a single token to the prediction gives an EM of zero. On the\n",
        "other hand, the F1\n",
        "\n",
        "-score can fail to catch truly incorrect answers. For example, if our\n",
        "\n",
        "predicted answer span is ‚Äúabout 6000 dollars‚Äù, then we get:"
      ],
      "metadata": {
        "id": "mxDUBHUBPhAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = \"about 6000 dollars\"\n",
        "print(f\"EM: {compute_exact(label, pred)}\")\n",
        "print(f\"F1: {compute_f1(label, pred)}\")"
      ],
      "metadata": {
        "id": "Iw3tRaAcPiaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relying on just the F1\n",
        "\n",
        "-score is thus misleading, and tracking both metrics is a good\n",
        "strategy to balance the trade-off between underestimating (EM) and overestimating\n",
        "(F1\n",
        "-score) model performance."
      ],
      "metadata": {
        "id": "Xsh9kDCcPkar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the reader we‚Äôll create a new pipeline with two nodes: a reader node and a\n",
        "node to evaluate the reader. We‚Äôll use the EvalReader class that takes the predictions\n",
        "from the reader and computes the corresponding EM and F1\n",
        "\n",
        "scores. To compare with\n",
        "the SQuAD evaluation, we‚Äôll take the best answers for each query with the top_1_em\n",
        "and top_1_f1 metrics that are stored in EvalAnswers:"
      ],
      "metadata": {
        "id": "jkN0GY9VPrsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from haystack.pipelines import Pipeline\n",
        "def evaluate_reader(reader):\n",
        "    score_keys = ['exact_match', 'f1']\n",
        "    p = Pipeline()\n",
        "    p.add_node(component=reader, name=\"Reader\", inputs=[\"Query\"])\n",
        "\n",
        "    eval_result = p.eval(\n",
        "        labels=labels_agg,\n",
        "        documents= [[label.document for label in multilabel.labels] for multilabel in labels_agg],\n",
        "        params={},\n",
        "    )\n",
        "    metrics = eval_result.calculate_metrics(simulated_top_k_reader=1)\n",
        "\n",
        "    return {k:v for k,v in metrics[\"Reader\"].items() if k in score_keys}\n",
        "\n",
        "reader_eval = {}\n",
        "reader_eval[\"Fine-tune on SQuAD\"] = evaluate_reader(reader)"
      ],
      "metadata": {
        "id": "M0nqaIIcP0Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that we specified skip_incorrect_retrieval=False. This is to ensure that\n",
        "the retriever always passes the context to the reader (as in the SQuAD evaluation).\n",
        "Now that we‚Äôve run every question through the reader, let‚Äôs print the scores:"
      ],
      "metadata": {
        "id": "QOZn6gtbP8sV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FZXTMPdNQCei"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4HS_QB5TQD_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_reader_eval(reader_eval):\n",
        "    fig, ax = plt.subplots()\n",
        "    df = pd.DataFrame.from_dict(reader_eval).reindex([\"exact_match\", \"f1\"])\n",
        "    df.plot(kind=\"bar\", ylabel=\"Score\", rot=0, ax=ax)\n",
        "    ax.set_xticklabels([\"EM\", \"F1\"])\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "plot_reader_eval(reader_eval)"
      ],
      "metadata": {
        "id": "GjY2lcjQP1_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, it seems that the fine-tuned model performs significantly worse on SubjQA than\n",
        "on SQuAD 2.0, where MiniLM achieves EM and F1\n",
        "\n",
        "scores of 76.1 and 79.5, respec‚Äê\n",
        "tively. One reason for the performance drop is that customer reviews are quite differ‚Äê\n",
        "ent from the Wikipedia articles the SQuAD 2.0 dataset is generated from, and the\n",
        "language they use is often informal. Another factor is likely the inherent subjectivity\n",
        "of our dataset, where both questions and answers differ from the factual information\n",
        "contained in Wikipedia. Let‚Äôs look at how to fine-tune a model on a dataset to get bet‚Äê\n",
        "ter results with domain adaptation."
      ],
      "metadata": {
        "id": "vjcnetiEQGgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Domain Adaptation\n",
        "\n",
        "Although models that are fine-tuned on SQuAD will often generalize well to other\n",
        "domains, we‚Äôve seen that for SubjQA the EM and F1\n",
        "\n",
        "scores of our model were much\n",
        "worse than for SQuAD. This failure to generalize has also been observed in other\n",
        "extractive QA datasets and is understood as evidence that transformer models are\n",
        "particularly adept at overfitting to SQuAD.15 The most straightforward way to\n",
        "improve the reader is by fine-tuning our MiniLM model further on the SubjQA train‚Äê\n",
        "ing set. The FARMReader has a train() method that is designed for this purpose and\n",
        "expects the data to be in SQuAD JSON format, where all the question-answer pairs\n",
        "are grouped together for each item"
      ],
      "metadata": {
        "id": "U1m2nZRjQIYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is quite a complex data format, so we‚Äôll need a few functions and some Pandas\n",
        "magic to help us do the conversion. The first thing we need to do is implement a\n",
        "function that can create the paragraphs array associated with each product ID. Each\n",
        "element in this array contains a single context (i.e., review) and a qas array of\n",
        "question-answer pairs. Here‚Äôs a function that builds up the paragraphs array:"
      ],
      "metadata": {
        "id": "T8SU0COcQTzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_paragraphs(df):\n",
        "    paragraphs = []\n",
        "    id2context = dict(zip(df[\"review_id\"], df[\"context\"]))\n",
        "    for review_id, review in id2context.items():\n",
        "        qas = []\n",
        "        # Filter for all question-answer pairs about a specific context\n",
        "        review_df = df.query(f\"review_id == '{review_id}'\")\n",
        "        id2question = dict(zip(review_df[\"id\"], review_df[\"question\"]))\n",
        "        # Build up the qas array\n",
        "        for qid, question in id2question.items():\n",
        "            # Filter for a single question ID\n",
        "            question_df = df.query(f\"id == '{qid}'\").to_dict(orient=\"list\")\n",
        "            ans_start_idxs = question_df[\"answers.answer_start\"][0].tolist()\n",
        "            ans_text = question_df[\"answers.text\"][0].tolist()\n",
        "            # Fill answerable questions\n",
        "            if len(ans_start_idxs):\n",
        "                answers = [\n",
        "                    {\"text\": text, \"answer_start\": answer_start}\n",
        "                    for text, answer_start in zip(ans_text, ans_start_idxs)]\n",
        "                is_impossible = False\n",
        "            else:\n",
        "                answers = []\n",
        "                is_impossible = True\n",
        "            # Add question-answer pairs to qas\n",
        "            qas.append({\"question\": question, \"id\": qid,\n",
        "                        \"is_impossible\": is_impossible, \"answers\": answers})\n",
        "        # Add context and question-answer pairs to paragraphs\n",
        "        paragraphs.append({\"qas\": qas, \"context\": review})\n",
        "    return paragraphs"
      ],
      "metadata": {
        "id": "Q2U1JNepQVC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, when we apply to the rows of a DataFrame associated with a single product ID,\n",
        "we get the SQuAD format:"
      ],
      "metadata": {
        "id": "MnrHdKuKRIyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "product = dfs[\"train\"].query(\"title == 'B00001P4ZH'\")\n",
        "create_paragraphs(product)"
      ],
      "metadata": {
        "id": "9-AzLx7-QbVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final step is to then apply this function to each product ID in the DataFrame of\n",
        "each split. The following convert_to_squad() function does this trick and stores the\n",
        "result in an electronics-{split}.json file:"
      ],
      "metadata": {
        "id": "-jkiHZSDRLZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "\n",
        "def convert_to_squad(dfs):\n",
        "    for split, df in dfs.items():\n",
        "        subjqa_data = {}\n",
        "        # Create `paragraphs` for each product ID\n",
        "        groups = (df.groupby(\"title\").apply(create_paragraphs)\n",
        "            .to_frame(name=\"paragraphs\").reset_index())\n",
        "        subjqa_data[\"data\"] = groups.to_dict(orient=\"records\")\n",
        "        # Save the result to disk\n",
        "        with open(f\"electronics-{split}.json\", \"w+\", encoding=\"utf-8\") as f:\n",
        "            json.dump(subjqa_data, f)\n",
        "\n",
        "convert_to_squad(dfs)"
      ],
      "metadata": {
        "id": "UtScGpVZQeDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the splits in the right format, let‚Äôs fine-tune our reader by specify‚Äê\n",
        "ing the locations of the train and dev splits, along with where to save the fine-tuned\n",
        "model:"
      ],
      "metadata": {
        "id": "ZZ2IaXx2ROjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_filename = \"electronics-train.json\"\n",
        "dev_filename = \"electronics-validation.json\"\n",
        "\n",
        "reader.train(data_dir=\".\", use_gpu=True, n_epochs=1, batch_size=16,\n",
        "             train_filename=train_filename, dev_filename=dev_filename)"
      ],
      "metadata": {
        "id": "4jLJHBbjQfaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader_eval[\"Fine-tune on SQuAD + SubjQA\"] = evaluate_reader(reader)\n"
      ],
      "metadata": {
        "id": "ats3Qz--Qguz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow, domain adaptation has increased our EM score by a factor of six and more than\n",
        "doubled the F1\n",
        "\n",
        "-score! At this point, you might be wondering why we didn‚Äôt just fine-\n",
        "tune a pretrained language model directly on the SubjQA training set. One reason is\n",
        "\n",
        "that we only have 1,295 training examples in SubjQA while SQuAD has over 100,000,\n",
        "so we might run into challenges with overfitting. Nevertheless, let‚Äôs take a look at what\n",
        "naive fine-tuning produces."
      ],
      "metadata": {
        "id": "LxcW978GRXOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a fair comparison, we‚Äôll use the same language model that was used for fine-tuning our baseline on SQuAD. As before, we‚Äôll load up the\n",
        "model with the FARMReader:"
      ],
      "metadata": {
        "id": "nid7Aym7RaQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "minilm_ckpt = \"microsoft/MiniLM-L12-H384-uncased\"\n",
        "minilm_reader = FARMReader(model_name_or_path=minilm_ckpt, progress_bar=False,\n",
        "                           max_seq_len=max_seq_length, doc_stride=doc_stride,\n",
        "                           return_no_answer=True)"
      ],
      "metadata": {
        "id": "zN71DyY4QitP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we fine-tune for one epoch:"
      ],
      "metadata": {
        "id": "Ey4W1H3RReOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "minilm_reader.train(data_dir=\".\", use_gpu=True, n_epochs=1, batch_size=16,\n",
        "             train_filename=train_filename, dev_filename=dev_filename)"
      ],
      "metadata": {
        "id": "-d9TbS1dQkAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and include the evaluation on the test set:"
      ],
      "metadata": {
        "id": "G_addLgCRgJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reader_eval[\"Fine-tune on SubjQA\"] = evaluate_reader(minilm_reader)\n"
      ],
      "metadata": {
        "id": "ZGAk0WZoQlXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_reader_eval(reader_eval)\n"
      ],
      "metadata": {
        "id": "r5FbQghBQmhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that fine-tuning the language model directly on SubjQA results in consid‚Äê\n",
        "erably worse performance than fine-tuning on SQuAD and SubjQA."
      ],
      "metadata": {
        "id": "1ACSZBhXRja_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the Whole QA Pipeline\n"
      ],
      "metadata": {
        "id": "eXQbTUV6QoU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we‚Äôve seen how to evaluate the reader and retriever components individu‚Äê\n",
        "ally, let‚Äôs tie them together to measure the overall performance of our pipeline. To do\n",
        "so, we‚Äôll need to augment our retriever pipeline with nodes for the reader and its evaluation. We‚Äôve seen that we get almost perfect recall at k = 10, so we can fix this\n",
        "value and assess the impact this has on the reader‚Äôs performance (since it will now\n",
        "receive multiple contexts per query compared to the SQuAD-style evaluation):"
      ],
      "metadata": {
        "id": "tWG0XAKbRpP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.pipelines import ExtractiveQAPipeline\n",
        "pipe = ExtractiveQAPipeline(retriever=bm25_retriever, reader=reader)\n",
        "\n",
        "# Evaluate!\n",
        "eval_result = pipe.eval(\n",
        "    labels=labels_agg,\n",
        "    params={},\n",
        ")\n",
        "metrics = eval_result.calculate_metrics(simulated_top_k_reader=1)\n",
        "# Extract metrics from reader\n",
        "reader_eval[\"QA Pipeline (top-1)\"] = {\n",
        "    k:v for k,v in metrics[\"Reader\"].items()\n",
        "    if k in [\"exact_match\", \"f1\"]}"
      ],
      "metadata": {
        "id": "WsCF1vDAQw65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#id reader-vs-pipeline\n",
        "#caption Comparison of EM and _F_~1~ scores for the reader against the whole QA pipeline\n",
        "plot_reader_eval({\"Reader\": reader_eval[\"Fine-tune on SQuAD + SubjQA\"],\n",
        "                  \"QA pipeline (top-1)\": reader_eval[\"QA Pipeline (top-1)\"]})"
      ],
      "metadata": {
        "id": "mY0qnwekQy6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then compare the top 1 EM and F1\n",
        "\n",
        "scores for the model to predict an answer\n",
        "\n",
        "in the documents returned by the retriever in Figure"
      ],
      "metadata": {
        "id": "KA0AeNeDRvDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Or get QA pipeline and Reader metrics in one shot:\n",
        "# Reader evaluation is run a second time using simulated perfect retriever results\n",
        "eval_result = pipe.eval(\n",
        "    labels=labels_agg,\n",
        "    params={},\n",
        "    add_isolated_node_eval=True\n",
        ")\n",
        "metrics = eval_result.calculate_metrics(simulated_top_k_reader=1)\n",
        "# Extract metrics from reader run in isolation with simulated perfect retriever\n",
        "isolated_metrics = eval_result.calculate_metrics(simulated_top_k_reader=1, eval_mode=\"isolated\")\n",
        "\n",
        "pipeline_reader_eval = {}\n",
        "pipeline_reader_eval[\"Reader\"] = {\n",
        "    k:v for k,v in isolated_metrics[\"Reader\"].items()\n",
        "    if k in [\"exact_match\", \"f1\"]}\n",
        "pipeline_reader_eval[\"QA Pipeline (top-1)\"] = {\n",
        "    k:v for k,v in metrics[\"Reader\"].items()\n",
        "    if k in [\"exact_match\", \"f1\"]}\n",
        "\n",
        "plot_reader_eval(pipeline_reader_eval)"
      ],
      "metadata": {
        "id": "9s7mepERQ1Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this plot we can see the effect that the retriever has on the overall performance.\n",
        "\n",
        "In particular, there is an overall degradation compared to matching the question-\n",
        "context pairs, as is done in the SQuAD-style evaluation. This can be circumvented by\n",
        "\n",
        "increasing the number of possible answers that the reader is allowed to predict."
      ],
      "metadata": {
        "id": "wZXer6oQRzto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval-Augmented Generation"
      ],
      "metadata": {
        "id": "-FS76rQmQ3J4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section\n",
        "we‚Äôll briefly touch on the current state of the art: retrieval-augmented generation"
      ],
      "metadata": {
        "id": "ItAzajrVR_MY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG extends the classic retriever-reader architecture that we‚Äôve seen in this chapter\n",
        "by swapping the reader for a generator and using DPR as the retriever. The generator\n",
        "is a pretrained sequence-to-sequence transformer like T5 or BART that receives latent\n",
        "vectors of documents from DPR and then iteratively generates an answer based on\n",
        "the query and these documents. Since DPR and the generator are differentiable, the\n",
        "whole process can be fine-tuned end-to-end"
      ],
      "metadata": {
        "id": "mTIexrFLSFJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To show RAG in action we‚Äôll use the DPRetriever from earlier, so we just need to\n",
        "instantiate a generator. There are two types of RAG models to choose from:"
      ],
      "metadata": {
        "id": "3G0OezX2SHcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. RAG-Sequence\n",
        "Uses the same retrieved document to generate the complete answer. In particular,\n",
        "the top k documents from the retriever are fed to the generator, which produces\n",
        "an output sequence for each document, and the result is marginalized to obtain\n",
        "the best answer."
      ],
      "metadata": {
        "id": "ftkau6qCSJ4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. RAG-Token\n",
        "Can use a different document to generate each token in the answer. This allows\n",
        "the generator to synthesize evidence from multiple documents."
      ],
      "metadata": {
        "id": "LxKo4Nm6SM9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since RAG-Token models tend to perform better than RAG-Sequence ones, we‚Äôll use\n",
        "the token model that was fine-tuned on NQ as our generator. Instantiating a genera‚Äê\n",
        "tor in Haystack is similar to instantiating the reader, but instead of specifying the\n",
        "max_seq_length and doc_stride parameters for a sliding window over the contexts,\n",
        "we specify hyperparameters that control the text generation:"
      ],
      "metadata": {
        "id": "3HhE4dX8SRkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from haystack.nodes import RAGenerator\n",
        "\n",
        "generator = RAGenerator(model_name_or_path=\"facebook/rag-token-nq\",\n",
        "                        embed_title=False, num_beams=5)"
      ],
      "metadata": {
        "id": "zMpLuaPYQ5SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next thing to do is tie"
      ],
      "metadata": {
        "id": "7XGsPoBTSSPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from haystack.pipelines import GenerativeQAPipeline\n",
        "\n",
        "pipe = GenerativeQAPipeline(generator=generator, retriever=dpr_retriever)"
      ],
      "metadata": {
        "id": "HrdayJ-6Q63q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs now give RAG a spin by feeding in some queries about the Amazon Fire tablet\n",
        "from before. To simplify the querying, we‚Äôll write a simple function that takes the\n",
        "query and prints out the top answers:"
      ],
      "metadata": {
        "id": "UNvSUIEPSZdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answers(query, top_k_generator=3):\n",
        "    preds = pipe.run(query=query,\n",
        "                     params={\"Retriever\": {\"top_k\":5,\n",
        "                                  \"filters\":{\"item_id\": [\"B0074BW614\"]}},\n",
        "                             \"Generator\": {\"top_k\": top_k_generator}})\n",
        "    print(f\"Question: {preds['query']} \\n\")\n",
        "    for idx in range(top_k_generator):\n",
        "        print(f\"Answer {idx+1}: {preds['answers'][idx].answer}\")"
      ],
      "metadata": {
        "id": "OeOgAdiIQ8qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_answers(query)\n"
      ],
      "metadata": {
        "id": "i2DxutiaQ97E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_answers(\"What is the main drawback?\")\n"
      ],
      "metadata": {
        "id": "MRzyxo9aQ_Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get better results we could fine-tune RAG end-to-end on\n",
        "SubjQA; we‚Äôll leave this as an exercise, but if you‚Äôre interested in exploring it there are\n",
        "scripts in the Transformers repository to help you get started."
      ],
      "metadata": {
        "id": "MWNaYvDcTzgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "Well, that was a whirlwind tour of QA, and you probably have many more questions\n",
        "that you‚Äôd like answered (pun intended!). In this chapter, we discussed two\n",
        "approaches to QA (extractive and generative) and examined two different retrieval\n",
        "algorithms (BM25 and DPR). Along the way, we saw that domain adaptation can be a\n",
        "simple technique to boost the performance of our QA system by a significant margin,\n",
        "and we looked at a few of the most common metrics that are used for evaluating such\n",
        "systems. Although we focused on closed-domain QA (i.e., a single domain of elec‚Äê\n",
        "\n",
        "tronic products), the techniques in this chapter can easily be generalized to the open-\n",
        "domain case; we recommend reading Cloudera‚Äôs excellent Fast Forward QA series to\n",
        "\n",
        "see what‚Äôs involved."
      ],
      "metadata": {
        "id": "gtVvB7wCT066"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deploying QA systems in the wild can be a tricky business to get right, and our expe‚Äê\n",
        "rience is that a significant part of the value comes from first providing end users with\n",
        "useful search capabilities, followed by an extractive component. In this respect, the\n",
        "reader can be used in novel ways beyond answering on-demand user queries. For\n",
        "example, researchers at Grid Dynamics were able to use their reader to automatically\n",
        "extract a set of pros and cons for each product in a client‚Äôs catalog. They also showed\n",
        "that a reader can be used to extract named entities in a zero-shot fashion by creating\n",
        "queries like ‚ÄúWhat kind of camera?‚Äù Given its infancy and subtle failure modes, we\n",
        "recommend exploring generative QA only once the other two approaches have been\n",
        "exhausted. This ‚Äúhierarchy of needs‚Äù for tackling QA problems"
      ],
      "metadata": {
        "id": "GjAuZbx6T3Xg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking ahead, one exciting research area is multimodal QA, which involves QA over\n",
        "multiple modalities like text, tables, and images. As described in the MultiModalQA\n",
        "benchmark,17 such systems could enable users to answer complex questions that inte‚Äê\n",
        "grate information across different modalities, like ‚ÄúWhen was the famous painting\n",
        "with two touching fingers completed?‚Äù Another area with practical business applica‚Äê\n",
        "\n",
        "tions is QA over a knowledge graph, where the nodes of the graph correspond to real-\n",
        "world entities and their relations are defined by the edges. By encoding factoids as\n",
        "\n",
        "(subject, predicate, object) triples, one can use the graph to answer questions about a\n",
        "missing element. For an example that combines transformers with knowledge graphs,\n",
        "see the Haystack tutorials. One more promising direction is automatic question gener‚Äê\n",
        "ation as a way to do some form of unsupervised/weakly supervised training using\n",
        "unlabeled data or data augmentation. Two recent examples include the papers on the\n",
        "Probably Answered Questions (PAQ) benchmark and synthetic data augmentation\n",
        "for cross-lingual settings.18"
      ],
      "metadata": {
        "id": "2KwOfOPrT5bH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l5lEsljwUFa_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this chapter we‚Äôve seen that in order to successfully use QA models for real-world\n",
        "use cases we need to apply a few tricks, such as implementing a fast retrieval pipeline\n",
        "to make predictions in near real time. Still, applying a QA model to a handful of pre‚Äê\n",
        "selected documents can take a couple of seconds on production hardware. Although\n",
        "this may not sound like much, imagine how different your experience would be if you\n",
        "had to wait a few seconds to get the results of a Google search‚Äîa few seconds of wait\n",
        "time can decide the fate of your transformer-powered application. In the next chapter\n",
        "we‚Äôll have a look at a few methods to accelerate model predictions further."
      ],
      "metadata": {
        "id": "GjYisSI7UKhj"
      }
    }
  ]
}