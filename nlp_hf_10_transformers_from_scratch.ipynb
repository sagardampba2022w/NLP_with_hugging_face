{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagardampba2022w/NLP_with_hugging_face/blob/main/nlp_hf_10_transformers_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drUlywcm0UTD"
      },
      "outputs": [],
      "source": [
        "# Uncomment and run this cell if you're on Colab or Kaggle\n",
        "# !git clone https://github.com/nlp-with-transformers/notebooks.git\n",
        "# %cd notebooks\n",
        "# from install import *\n",
        "# install_requirements(is_chapter10=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui_OfGXz0UTG"
      },
      "outputs": [],
      "source": [
        "# hide\n",
        "from utils import *\n",
        "setup_chapter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfneUrod0UTH"
      },
      "source": [
        "# Training Transformers from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this chapter, we’ll close the circle and build our very own GPT-like\n",
        "\n",
        "model for generating Python source code! We call the resulting model CodeParrot.\n",
        "So far we’ve mostly worked on data-constrained applications where the amount of\n",
        "labeled training data is limited. In these cases, transfer learning helped us build per‐\n",
        "formant models. We took transfer learning to the limit in Chapter 9, where we barely\n",
        "used any training data at all.\n",
        "In this chapter we’ll move to the other extreme and look at what we can do when we\n",
        "are drowning in all the data we could possibly want. We’ll explore the pretraining step\n",
        "itself and learn how to train a transformer from scratch. In working through this\n",
        "problem, we’ll look at some aspects of training that we have not considered yet, such\n",
        "as the following:\n",
        "• Gathering and processing a very large dataset\n",
        "• Creating a custom tokenizer for our dataset\n",
        "• Training a model on multiple GPUs at scale\n",
        "To efficiently train large models with billions of parameters, we’ll need special tools\n",
        "for distributed training. Although the Trainer from Transformers supports dis‐\n",
        "tributed training, we’ll take the opportunity to showcase a powerful PyTorch library called Accelerate. We’ll end up touching on some of the largest NLP models in use\n",
        "today—but first, we need to find a sufficiently large dataset."
      ],
      "metadata": {
        "id": "h1ixHfwE0kBg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3EQv-Bb0UTI"
      },
      "source": [
        "> **Note:** In this chapter a large dataset and the script to train a large language model on a distributed infrastructure are built. As such not all the steps in this notebook are executable on platforms such as Colab or Kaggle. Either downscale the steps at critical points or use this notebook as an inspiration when building a script for distributed training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUA5i4Ry0UTI"
      },
      "source": [
        "## Large Datasets and Where to Find Them"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many domains where you may actually have a large amount of data at\n",
        "hand, ranging from legal documents to biomedical datasets to programming codeba‐\n",
        "ses. In most cases, these datasets are unlabeled, and their large size means that they\n",
        "can usually only be labeled through the use of heuristics, or by using accompanying\n",
        "metadata that is stored during the gathering process.\n",
        "Nevertheless, a very large corpus can be useful even when it is unlabeled or only heu‐\n",
        "ristically labeled. We saw an example of this in Chapter 9, where we used the unla‐\n",
        "beled part of a dataset to fine-tune a language model for domain adaptation. This\n",
        "approach typically yields a performance gain when limited data is available. The deci‐\n",
        "sion to train from scratch rather than fine-tune an existing model is mostly dictated\n",
        "by the size of your fine-tuning corpus and the domain differences between the avail‐\n",
        "able pretrained models and the corpus.\n",
        "Using a pretrained model forces you to use the model’s corresponding tokenizer, but\n",
        "using a tokenizer that is trained on a corpus from another domain is typically subop‐\n",
        "timal. For example, using GPT’s pretrained tokenizer on legal documents, other lan‐\n",
        "guages, or even completely different sequences such as musical notes or DNA\n",
        "sequences will result in poor tokenization (as we will see shortly).\n",
        "\n",
        "As the amount of training data you have access to gets closer to the amount of data\n",
        "used for pretraining, it thus becomes interesting to consider training the model and\n",
        "the tokenizer from scratch, provided the necessary computational resources are avail‐\n",
        "able. Before we discuss the different pretraining objectives further, we first need to\n",
        "build a large corpus suitable for pretraining. Building such a corpus comes with its\n",
        "own set of challenges, which we’ll explore in the next section."
      ],
      "metadata": {
        "id": "ptoUxdEO0r39"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JaF-23Q0UTJ"
      },
      "source": [
        "### Challenges of Building a Large-Scale Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The quality of a model after pretraining largely reflects the quality of the pretraining\n",
        "corpus. In particular, the model will inherit any defects in the pretraining corpus.\n",
        "Thus, before we attempt to create one of our own it’s good to be aware of some of the common issues and challenges that are associated with building large corpora for\n",
        "pretraining.\n",
        "As the dataset gets larger and larger, the chances that you can fully control—or at\n",
        "least have a precise idea of—what is inside it diminish. A very large dataset will most\n",
        "likely not have been assembled by dedicated creators that craft one example at a time,\n",
        "while being aware and knowledgeable of the full pipeline and the task that the\n",
        "machine learning model will be applied to. Instead, it is much more likely that a very\n",
        "large dataset will have been created in an automatic or semiautomatic way by collect‐\n",
        "ing data that is generated as a side effect of other activities. For instance, it may con‐\n",
        "sist of all the documents (e.g., contracts, purchase orders, etc.) that a company stores,\n",
        "logs from user activities, or data gathered from the internet.\n",
        "There are several important consequences that follow from the fact that large-scale\n",
        "datasets are mostly created with a high degree of automation. An important consider‐\n",
        "ation is that there is limited control over both their content and the way they are cre‐\n",
        "ated, and thus the risk of training a model on biased and lower-quality data increases.\n",
        "Recent investigations of famous large-scale datasets like BookCorpus and C4, which\n",
        "were used to train BERT and T5, respectively, have uncovered (among other things)\n",
        "that:1\n",
        "\n",
        "• A significant proportion of the C4 corpus is machine-translated rather than\n",
        "translated by humans.\n",
        "\n",
        "• Disparate erasure of African-American English as a result of stopword filtering\n",
        "in C4 has resulted in an underrepresentation of such content.\n",
        "\n",
        "• It is typically difficult in a large text corpus to find a middle ground between\n",
        "including (often too much) sexually or other explicit content and totally erasing\n",
        "all mention of sexuality or gender. As a surprising consequence of this, a rather\n",
        "common word like “sex” (which can have both neutral and explicit meanings) is\n",
        "completely unknown to a tokenizer that is trained on C4, since this word is fully\n",
        "absent from the corpus.\n",
        "\n",
        "• There are many occurrences of copyright violation in BookCorpus, and probably\n",
        "in other large-scale datasets as well.2\n",
        "\n",
        "• There is genre skew toward “romance” novels in BookCorpus."
      ],
      "metadata": {
        "id": "5hlf8tBK0wRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CMYkqrZU1c_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s illustrate the notion of a model being skewed by the data by comparing text gen‐\n",
        "erations from GPT and GPT-2. GPT was mostly trained on BookCorpus, while\n",
        "GPT-2 was trained on web pages, blogs, and news articles linked from Reddit. We’ll\n",
        "compare similar-sized versions of both models on the same prompt, so that the main\n",
        "difference is the pretraining dataset, and we’ll use the text-generation pipeline to\n",
        "investigate the model outputs:"
      ],
      "metadata": {
        "id": "bX-it97b1mwX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWbJ_qod0UTJ"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "from transformers import pipeline, set_seed\n",
        "\n",
        "generation_gpt = pipeline(\"text-generation\", model=\"openai-gpt\")\n",
        "generation_gpt2 = pipeline(\"text-generation\", model=\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U8Or-580UTK",
        "outputId": "0c67ad4a-cf42-450e-bd68-5d980fd7f792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT  size: 116.5M parameters\n",
            "GPT2 size: 124.4M parameters\n"
          ]
        }
      ],
      "source": [
        "def model_size(model):\n",
        "    return sum(t.numel() for t in model.parameters())\n",
        "\n",
        "print(f\"GPT  size: {model_size(generation_gpt.model)/1000**2:.1f}M parameters\")\n",
        "print(f\"GPT2 size: {model_size(generation_gpt2.model)/1000**2:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original GPT model is about the same size as the smallest GPT-2 model. Now we\n",
        "can generate three different completions from each model, each with the same input\n",
        "prompt:"
      ],
      "metadata": {
        "id": "tzPEyeoF1rd4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx2ZUUK20UTM"
      },
      "outputs": [],
      "source": [
        "# hide\n",
        "set_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZYDk9Is0UTM",
        "outputId": "62d70ae1-00d3-43ce-f1df-80e0d510ad96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT completions:\n",
            "1.\n",
            "When they came back.\n",
            " \" we need all we can get, \" jason said once they had settled into the back of\n",
            "the truck without anyone stopping them. \" after getting out here, it 'll be up\n",
            "to us what to find. for now\n",
            "2.\n",
            "When they came back.\n",
            " his gaze swept over her body. he 'd dressed her, too, in the borrowed clothes\n",
            "that she 'd worn for the journey.\n",
            " \" i thought it would be easier to just leave you there. \" a woman like\n",
            "3.\n",
            "When they came back to the house and she was sitting there with the little boy.\n",
            " \" don't be afraid, \" he told her. she nodded slowly, her eyes wide. she was so\n",
            "lost in whatever she discovered that tom knew her mistake\n",
            "\n",
            "GPT-2 completions:\n",
            "1.\n",
            "When they came back we had a big dinner and the other guys went to see what\n",
            "their opinion was on her. I did an hour and they were happy with it.\n",
            "2.\n",
            "When they came back to this island there had been another massacre, but he could\n",
            "not help but feel pity for the helpless victim who had been left to die, and\n",
            "that they had failed that day. And so was very, very grateful indeed.\n",
            "3.\n",
            "When they came back to our house after the morning, I asked if she was sure. She\n",
            "said, \"Nope.\" The two kids were gone that morning. I thought they were back to\n",
            "being a good friend.\n",
            "\n",
            "When Dost\n"
          ]
        }
      ],
      "source": [
        "def enum_pipeline_ouputs(pipe, prompt, num_return_sequences):\n",
        "    out = pipe(prompt, num_return_sequences=num_return_sequences,\n",
        "               clean_up_tokenization_spaces=True)\n",
        "    return \"\\n\".join(f\"{i+1}.\" + s[\"generated_text\"] for i, s in enumerate(out))\n",
        "\n",
        "prompt = \"\\nWhen they came back\"\n",
        "print(\"GPT completions:\\n\" + enum_pipeline_ouputs(generation_gpt, prompt, 3))\n",
        "print(\"\")\n",
        "print(\"GPT-2 completions:\\n\" + enum_pipeline_ouputs(generation_gpt2, prompt, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By just sampling a handful of outputs from both models we can already see the dis‐\n",
        "tinctive “romance” skew in GPT generation, which will typically imagine a dialogue\n",
        "with a romantic interaction between a woman and a man. On the other hand, GPT-2\n",
        "was trained on webtext linked to and from Reddit articles and mostly adopts a neu‐\n",
        "tral “they” in its generations, which contain “blog-like” or adventure-related elements."
      ],
      "metadata": {
        "id": "NxPFEmHL1xyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, any model trained on a dataset will reflect the language bias and over- or\n",
        "underrepresentation of populations and events in its training data. These biases in the\n",
        "behavior of the model are important to take into consideration with regard to the tar‐\n",
        "get audience interacting with the model; for some useful guidelines, we refer you to a\n",
        "paper by Google that provides a framework for dataset development.3\n",
        "This brief introduction should give you an idea of the difficult challenges you face\n",
        "when creating large text corpora. With these in mind, let’s now take a look at creating\n",
        "our own dataset!"
      ],
      "metadata": {
        "id": "n2KFtqrC10tc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J2zdJEk0UTN"
      },
      "source": [
        "### Building a Custom Code Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To simplify the task a bit, we’ll focus on building a code generation model for the\n",
        "Python programming language only.4\n",
        "\n",
        "The first thing we’ll need is a large pretraining\n",
        "corpus consisting of Python source code. Fortunately, there is a natural resource that\n",
        "every software engineer knows: GitHub! The famous code-sharing website hosts\n",
        "terabytes of code repositories that are openly accessible and can be downloaded and\n",
        "used according to their respective licenses. At the time of this book’s writing, GitHub\n",
        "\n",
        "hosts more than 20 million code repositories. Many of them are small or test reposi‐\n",
        "tories created by users for learning, future side projects, or testing purposes.\n",
        "GitHub repositories can be accessed in two main ways:\n",
        "• Via the GitHub REST API, like we saw in Chapter 9 when we downloaded all the\n",
        "GitHub issues of the Transformers repository\n",
        "• Via public dataset inventories like Google BigQuery\n",
        "\n",
        "Since the REST API is rate limited and we need a lot data for our pretraining corpus,\n",
        "\n",
        "we’ll use Google BigQuery to extract all the Python repositories. The bigquery-\n",
        "public-data.github_repos.contents table contains copies of all ASCII files that are\n",
        "\n",
        "less than 10 MB in size. Projects also need to be open source to be included, as deter‐\n",
        "mined by GitHub’s License API."
      ],
      "metadata": {
        "id": "4fC48OHG14Tu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwf3aGR-0UTN"
      },
      "source": [
        "#### Creating a dataset with Google BigQuery"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll begin by extracting all the Python files in GitHub public repositories from the\n",
        "snapshot on Google BigQuery. For the sake of reproducibility and in case the policy\n",
        "around free usage of BigQuery changes in the future, we will also share this dataset\n",
        "on the Hugging Face Hub. The steps to export these files are adapted from the Trans‐\n",
        "Coder implementation and are as follows:5\n",
        "\n",
        "\n",
        "1. Create a Google Cloud account (a free trial should be sufficient).\n",
        "2. Create a Google BigQuery project under your account.\n",
        "3. In this project, create a dataset.\n",
        "4. In this dataset, create a table where the results of the SQL request will be stored.\n",
        "5. Prepare and run the following SQL query on the github_repos (to save the\n",
        "query results, select More > Query Options, check the “Set a destination table for\n",
        "query results” box, and specify the table name):\n",
        "\n",
        "`SELECT\n",
        "f.repo_name, f.path, c.copies, c.size, c.content, l.license\n",
        "FROM\n",
        "`bigquery-public-data.github_repos.files` AS f\n",
        "JOIN\n",
        "`bigquery-public-data.github_repos.contents` AS c\n",
        "ON\n",
        "f.id = c.id\n",
        "JOIN\n",
        "`bigquery-public-data.github_repos.licenses` AS l\n",
        "ON\n",
        "f.repo_name = l.repo_name\n",
        "WHERE\n",
        "NOT c.binary\n",
        "AND ((f.path LIKE '%.py')\n",
        "AND (c.size BETWEEN 1024\n",
        "AND 1048575))`"
      ],
      "metadata": {
        "id": "CFMtUkYs2DXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s8TjBknR2Njm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command processes about 2.6 TB of data to extract 26.8 million files. The result\n",
        "is a dataset of about 50 GB of compressed JSON files, each containing the source code\n",
        "of Python files. We filtered to remove empty files and small files such as __init__.py\n",
        "that don’t contain much useful information. We also filtered out files larger than 1\n",
        "MB, and we downloaded the licenses for all the files so we can filter the training data\n",
        "based on licenses if we want later on.\n",
        "Next, we’ll download the results to our local machine. If you try this at home, make\n",
        "sure you have good bandwidth available and at least 50 GB of free disk space. The\n",
        "easiest way to get the resulting table to your local machine is to follow this two-step\n",
        "process:\n",
        "\n",
        "1. Export your results to Google Cloud:\n",
        "a. Create a bucket and a folder in Google Cloud Storage (GCS).\n",
        "b. Export your table to this bucket by selecting Export > Export to GCS, with an\n",
        "export format of JSON and gzip compression.\n",
        "2. To download the bucket to your machine, use the gsutil library:\n",
        "a. Install gsutil with pip install gsutil.\n",
        "b. Configure gsutil with your Google account: gsutil config.\n",
        "c. Copy your bucket on your machine:\n",
        "\n",
        "`$ gsutil -m -o\n",
        "\"GSUtil:parallel_process_count=1\" cp -r gs://<name_of_bucket>`\n",
        "\n",
        "\n",
        "Alternatively, you can directly download the dataset from the Hugging Face Hub with\n",
        "the following command:\n",
        "\n",
        "`$ git clone https://huggingface.co/datasets/transformersbook/codeparrot`"
      ],
      "metadata": {
        "id": "bLCtmA_C2Qzm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0yYQvne0UTN"
      },
      "source": [
        "#sidebar To Filter the Noise or Not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJguznpx0UTN"
      },
      "source": [
        "### Working with Large Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading a very large dataset is often a challenging task, in particular when the data is\n",
        "larger than your machine’s RAM. For a large-scale pretraining dataset, this is a very\n",
        "common situation. In our example, we have 50 GB of compressed data and about 200\n",
        "GB of uncompressed data, which is difficult to extract and load into the RAM mem‐\n",
        "ory of a standard-sized laptop or desktop computer.\n",
        "Thankfully, Datasets has been designed from the ground up to overcome this\n",
        "problem with two specific features that allow you to set yourself free from RAM and\n",
        "hard drive space limitations: memory mapping and streaming."
      ],
      "metadata": {
        "id": "ed8Of-Gj2fgS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUjFngRw0UTO"
      },
      "source": [
        "#### Memory mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To overcome RAM limitations, Datasets uses a mechanism for zero-copy and zero-\n",
        "overhead memory mapping that is activated by default. Basically, each dataset is\n",
        "\n",
        "cached on the drive in a file that is a direct reflection of the content in RAM memory.\n",
        "Instead of loading the dataset in RAM, Datasets opens a read-only pointer to thisfile and uses it as a substitute for RAM, basically using the hard drive as a direct\n",
        "extension of the RAM memory.\n",
        "Up to now we have mostly used Datasets to access remote datasets on the Hugging\n",
        "Face Hub. Here, we will directly load our 50 GB of compressed JSON files that we\n",
        "have stored locally in the codeparrot repository. Since the JSON files are com‐\n",
        "pressed, we first need to decompress them, which Datasets takes care of for us. Be\n",
        "careful, because this requires about 180 GB of free disk space! However, it will use\n",
        "almost no RAM. By setting delete_extracted=True in the dataset’s downloading\n",
        "configuration, we can make sure that we delete all the files we don’t need anymore as\n",
        "soon as possible:"
      ],
      "metadata": {
        "id": "mX5jA2xK2je5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fibt2m220UTO"
      },
      "source": [
        "> **Note:** The following code block assumes that you have downloaded the BigQuery dataset to a folder called `codeparrot`. We suggest skipping this step since it will unpack the compressed files and require ~180GB of disk space. This code is just for demonstration purposes and you can just continue below with the streamed dataset which will not consume that much disk space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD1jX8cE0UTP"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "from datasets import load_dataset, DownloadConfig\n",
        "\n",
        "download_config = DownloadConfig(delete_extracted=True)\n",
        "dataset = load_dataset(\"./codeparrot\", split=\"train\",\n",
        "                       download_config=download_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8J0Vbm60UTP",
        "outputId": "3e17fe3b-6189-4927-9279-4aead1eb78e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of python files code in dataset : 18695559\n",
            "Dataset size (cache file) : 183.68 GB\n",
            "RAM memory used: 4924 MB\n"
          ]
        }
      ],
      "source": [
        "import psutil, os\n",
        "\n",
        "print(f\"Number of python files code in dataset : {len(dataset)}\")\n",
        "ds_size = sum(os.stat(f[\"filename\"]).st_size for f in dataset.cache_files)\n",
        "# os.stat.st_size is expressed in bytes, so we convert to GB\n",
        "print(f\"Dataset size (cache file) : {ds_size / 2**30:.2f} GB\")\n",
        "# Process.memory_info is expressed in bytes, so we convert to MB\n",
        "print(f\"RAM used: {psutil.Process(os.getpid()).memory_info().rss >> 20} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the dataset is much larger than our typical RAM memory, but we can\n",
        "still load and access it, and we’re actually using a very limited amount of memory.\n",
        "You may wonder if this will make our training I/O-bound. In practice, NLP data is\n",
        "usually very lightweight to load in comparison to the model processing computa‐\n",
        "tions, so this is rarely an issue. In addition, the zero-copy/zero-overhead format uses\n",
        "Apache Arrow under the hood, which makes it very efficient to access any element.\n",
        "Depending on the speed of your hard drive and the batch size, iterating over the\n",
        "dataset can typically be done at a rate of a few tenths of a GB/s to several GB/s. This is\n",
        "great, but what if you can’t free enough disk space to store the full dataset locally?\n",
        "Everybody knows the feeling of helplessness when you get a full disk warning and\n",
        "need to painfully try to reclaim a few GB by looking for hidden files to delete. Luckily,\n",
        "you don’t need to store the full dataset locally if you use the streaming feature of"
      ],
      "metadata": {
        "id": "zB9Qv1tu2vUb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw8Isvrx0UTP"
      },
      "source": [
        "#### Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some datasets (reaching up to 1 TB or more) will be difficult to fit even on a standard\n",
        "hard drive. In this case, an alternative to scaling up the server you are using is to\n",
        "stream the dataset. This is also possible with Datasets for a number of compressed\n",
        "or uncompressed file formats that can be read line by line, like JSON Lines, CSV, or\n",
        "text (either raw or zip, gzip, or zstandard compressed). Let’s load our dataset directly\n",
        "from the compressed JSON files instead of creating a cache file from them:"
      ],
      "metadata": {
        "id": "aAI_Lwn02zWI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gknsb5DU0UTQ",
        "outputId": "f0c4b196-f0aa-4c08-858e-a21661860ce0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration default-cae7a1d2f0dbde67\n"
          ]
        }
      ],
      "source": [
        "# hide_output\n",
        "streamed_dataset = load_dataset('./codeparrot', split=\"train\", streaming=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you’ll see, loading the dataset is instantaneous! In streaming mode, the com‐\n",
        "pressed JSON files will be opened and read on the fly. Our dataset is now an Iterable\n",
        "Dataset object. This means that we cannot access random elements of it, like\n",
        "streamed_dataset[1264], but we need to read it in order, for instance with\n",
        "next(iter(streamed_dataset)). It’s still possible to use methods like shuffle(), but\n",
        "these will operate by fetching a buffer of examples and shuffling within this buffer\n",
        "(the size of the buffer is adjustable). When several files are provided as raw files (like\n",
        "our 184 files here), shuffle() will also randomize the order of files for the iteration."
      ],
      "metadata": {
        "id": "xfhVzNau2217"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onbjy6lm0UTQ",
        "outputId": "276a4a8f-e28b-4f78-fad8-cf8223bd57f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "iterator = iter(streamed_dataset)\n",
        "\n",
        "print(dataset[0] == next(iterator))\n",
        "print(dataset[1] == next(iterator))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The samples of a streamed dataset are identical to the samples of a nonstreamed data‐\n",
        "set, as we can see:"
      ],
      "metadata": {
        "id": "RyIMjrg625Zj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main interest of using a streaming dataset is that loading this dataset will not cre‐\n",
        "ate a cache file on the drive or require any (significant) RAM memory. The original\n",
        "raw files are extracted and read on the fly when a new batch of examples is requested,\n",
        "and only that batch is loaded in memory. This reduces the memory footprint of our\n",
        "dataset from 180 GB to 50 GB. But we can take this one step further—instead of\n",
        "pointing to the local dataset we can reference the dataset on the Hub, and then\n",
        "directly download samples without downloading the raw files locally:"
      ],
      "metadata": {
        "id": "nyBcbuzx3A8O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDkJOlxc0UTR"
      },
      "outputs": [],
      "source": [
        "remote_dataset = load_dataset('transformersbook/codeparrot', split=\"train\",\n",
        "                              streaming=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset behaves exactly like the previous one, but behind the scenes downloads\n",
        "the examples on the fly. With such a setup, we can then use arbitrarily large datasets\n",
        "on an (almost) arbitrarily small server. Let’s push our dataset with a train and valida‐\n",
        "tion split to the Hugging Face Hub and access it with streaming."
      ],
      "metadata": {
        "id": "gRQRfczs3C9p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-p2NLZG0UTR"
      },
      "source": [
        "### Adding Datasets to the Hugging Face Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pushing our dataset to the Hugging Face Hub will allow us to:\n",
        "• Easily access it from our training server.\n",
        "• See how streaming datasets work seamlessly with datasets from the Hub.\n",
        "• Share it with the community, including you, dear reader!\n",
        "To upload the dataset, we first need to log in to our Hugging Face account by running\n",
        "the following command in the terminal and providing the relevant credentials:\n",
        "\n",
        "`$ huggingface-cli login`\n",
        "\n",
        "This is equivalent to the notebook_login() helper function we used in previous\n",
        "chapters. Once this is done, we can directly create a new dataset on the Hub and\n",
        "upload the compressed JSON files. To simplify things, we will create two repositories:\n",
        "one for the train split and one for the validation split. We can do this by running the\n",
        "repo create command of the CLI as follows:\n",
        "\n",
        "`$ huggingface-cli repo create --type dataset --organization transformersbook \\\n",
        "codeparrot-train`\n",
        "\n",
        "`$ huggingface-cli repo create --type dataset --organization transformersbook \\\n",
        "codeparrot-valid`"
      ],
      "metadata": {
        "id": "bF9M2X4N3FLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we’ve specified that the repository should be a dataset (in contrast to the model\n",
        "repositories used to store weights), along with the organization we’d like to store the\n",
        "repositories under. If you’re running this code under your personal account, you can\n",
        "omit the --organization flag. Next, we need to clone these empty repositories to our\n",
        "local machine, copy the JSON files to them, and push the changes to the Hub. We will\n",
        "take the last compressed JSON file out of the 184 we have as the validation file (i.e.,\n",
        "roughly 0.5 percent of our dataset). Execute these commands to clone the repository\n",
        "from the Hub to your local machine:\n",
        "\n",
        "`$ git clone https://huggingface.co/datasets/transformersbook/codeparrot-train`\n",
        "\n",
        "`$ git clone https://huggingface.co/datasets/transformersbook/codeparrot-valid`\n",
        "\n",
        "Next, copy all but the last GitHub file as the training set:\n",
        "`$ cd codeparrot-train`\n",
        "\n",
        "`$ cp ../codeparrot/*.json.gz .`\n",
        "\n",
        "`$ rm ./file-000000000183.json.gz`\n",
        "\n",
        "Then commit the files and push them to the Hub:\n",
        "\n",
        "`$ git add .`\n",
        "\n",
        "`$ git commit -m \"Adding dataset files\"`\n",
        "\n",
        "`$ git push`\n",
        "\n",
        "\n",
        "repeat for validation\n",
        "\n",
        "$ cd ../codeparrot-valid\n",
        "$ cp ../codeparrot/file-000000000183.json.gz .\n",
        "$ mv ./file-000000000183.json.gz ./file-000000000183_validation.json.gz\n",
        "$ git add .\n",
        "$ git commit -m \"Adding dataset files\"\n",
        "$ git push\n",
        "\n",
        "The git add . step can take a couple of minutes since a hash of all the files is com‐\n",
        "puted. Uploading all the files will also take a little while. Since this will enable us to\n",
        "use streaming later in the chapter, however, this is not lost time, and this step will\n",
        "allow us to go significantly faster in the rest of our experiments. Note that we added a\n",
        "_validation suffix to the validation filename. This will enable us to load it later as a\n",
        "validation split.\n",
        "And that’s it! Our two splits of the dataset as well as the full dataset are now live on\n",
        "the Hugging Face Hub at the following URLs:"
      ],
      "metadata": {
        "id": "5q3AwGbj3Ruk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JfaLjiC0UTR"
      },
      "source": [
        "## Building a Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, when we train a new model, using a tokenizer prepared for another dataset\n",
        "can be suboptimal. Here are a few examples of the kinds of problems we might run\n",
        "into when using an existing tokenizer:\n",
        "\n",
        "• The T5 tokenizer was trained on the C4 corpus that we encountered earlier, but\n",
        "an extensive step of stopword filtering was used to create it. As a result, the T5\n",
        "tokenizer has never seen common English words such as “sex.”\n",
        "• The CamemBERT tokenizer was also trained on a very large corpus of text, but\n",
        "only comprising French text (the French subset of the OSCAR corpus). As such,\n",
        "it is unaware of common English words such “being.”\n",
        "We can easily test these features of each tokenizer in practice:"
      ],
      "metadata": {
        "id": "rDxjNg0A3xgp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "29ced71e91434126970160a03cc006a5",
            "6f437f06babc4f01b5f02ed2e11a274f",
            "086fad17475145a2960cf393d6da4da5",
            "83b90218ddd54563b5abc524ed820741",
            "8fb62d059f0a4ab397767ec6497ab5ed",
            "b1fd604f95db44748a1b53a31be9ff5a"
          ]
        },
        "id": "vLdn538C0UTR",
        "outputId": "8982d640-dc68-464f-83fc-42b1f76ea3d3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29ced71e91434126970160a03cc006a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f437f06babc4f01b5f02ed2e11a274f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "086fad17475145a2960cf393d6da4da5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83b90218ddd54563b5abc524ed820741",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/508 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fb62d059f0a4ab397767ec6497ab5ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1fd604f95db44748a1b53a31be9ff5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.33M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# hide_output\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def tok_list(tokenizer, string):\n",
        "    input_ids = tokenizer(string, add_special_tokens=False)[\"input_ids\"]\n",
        "    return [tokenizer.decode(tok) for tok in input_ids]\n",
        "\n",
        "tokenizer_T5 = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "tokenizer_camembert = AutoTokenizer.from_pretrained(\"camembert-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRf-WEGL0UTS",
        "outputId": "bf2f9939-684d-432a-ee05-4d2911991fa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "T5 tokens for \"sex\": ['', 's', 'ex']\n",
            "CamemBERT tokens for \"being\": ['be', 'ing']\n"
          ]
        }
      ],
      "source": [
        "print(f'T5 tokens for \"sex\": {tok_list(tokenizer_T5,\"sex\")}')\n",
        "print(f'CamemBERT tokens for \"being\": {tok_list(tokenizer_camembert,\"being\")}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD284FNo0UTS"
      },
      "source": [
        "### The Tokenizer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you saw in Chapter 4, the tokenizer is a processing pipeline consisting of four\n",
        "steps: normalization, pretokenization, the tokenizer model, and postprocessing. The\n",
        "part of the tokenizer pipeline that can be trained on data is the tokenizer model. As\n",
        "we discussed in Chapter 2, there are several subword tokenization algorithms that can\n",
        "be used, such as BPE, WordPiece, and Unigram.\n",
        "\n",
        "\n",
        "1. BPE starts from a list of basic units (single characters) and creates a vocabulary by a\n",
        "process of progressively creating new tokens formed by merging the most frequently\n",
        "co-occurring basic units and adding them to the vocabulary. This process is reiterated\n",
        "until a predefined vocabulary size is reached.\n",
        "\n",
        "\n",
        "2. Unigram starts from the other end, by initializing its base vocabulary with all the\n",
        "words in the corpus, and potential subwords. Then it progressively removes or splits\n",
        "the less useful tokens to obtain a smaller and smaller vocabulary, until the target\n",
        "vocabulary size is reached. WordPiece is a predecessor of Unigram, and its official\n",
        "implementation was never open-sourced by Google.\n",
        "\n",
        "The impact of these various algorithms on downstream performance varies depend‐\n",
        "ing on the task, and overall it’s quite difficult to identify if one algorithm is clearly\n",
        "superior to the others. Both BPE and Unigram have reasonable performance in most\n",
        "cases, but let’s have a look at some aspects to consider when evaluating."
      ],
      "metadata": {
        "id": "cyUBTEs_36aY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXEXs2KL0UTS"
      },
      "source": [
        "### Measuring Tokenizer Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimality and performance of a tokenizer are challenging to measure in prac‐\n",
        "tice. Some possible metrics include:\n",
        "\n",
        "• Subword fertility, which calculates the average number of subwords produced per\n",
        "tokenized word\n",
        "\n",
        "• Proportion of continued words, which refers to the proportion of tokenized words\n",
        "in a corpus that are split into at least two subtokens\n",
        "\n",
        "• Coverage metrics like the proportion of unknown words or rarely used tokens in\n",
        "a tokenized corpus\n",
        "\n",
        "In addition, robustness to misspelling or noise is often estimated, as well as model\n",
        "performance on such out-of-domain examples, as this strongly depends on the toke‐\n",
        "nization process."
      ],
      "metadata": {
        "id": "wC0-L6ON7a_P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux466EZb0UTS"
      },
      "source": [
        "### A Tokenizer for Python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need a custom tokenizer for our use case: tokenizing Python code. The question\n",
        "of pretokenization merits some discussion for programming languages. If we split on\n",
        "whitespaces and remove them, we will lose all the indentation information, which in\n",
        "Python is important for the semantics of the program (just think about while loops,\n",
        "or if-then-else statements). On the other hand, line breaks are not meaningful and\n",
        "can be added or removed without impact on the semantics. Similarly, splitting on\n",
        "punctuation, like an underscore, which is used to compose a single variable name\n",
        "from several subparts, might not make as much sense as it would in natural language.\n",
        "Using a natural language pretokenizer for tokenizing code thus seems potentially sub‐\n",
        "optimal.\n",
        "Let’s see if there are any tokenizers in the collection provided on the Hub that might\n",
        "be useful to us. We want a tokenizer that preserves spaces, so a good candidate could\n",
        "be a byte-level tokenizer like the one from GPT-2. Let’s load this tokenizer and\n",
        "explore its tokenization properties:"
      ],
      "metadata": {
        "id": "a8BYMyE_7oIJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htWS9ZUR0UTT",
        "outputId": "1aac5596-c187-4f34-b336-224abe7bd19a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['def', 'Ġsay', '_', 'hello', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"',\n",
            "'Hello', ',', 'ĠWorld', '!\"', ')', 'Ġ#', 'ĠPrint', 'Ġit', 'Ċ', 'Ċ', 'say', '_',\n",
            "'hello', '()', 'Ċ']\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "python_code = r\"\"\"def say_hello():\n",
        "    print(\"Hello, World!\")\n",
        "# Print it\n",
        "say_hello()\n",
        "\"\"\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "print(tokenizer(python_code).tokens())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsvk4Q6M0UTU",
        "outputId": "5852336d-d36a-49aa-e961-cfa3ac1a4d1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.backend_tokenizer.normalizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the GPT-2 tokenizer uses no normalization. It works directly on\n",
        "the raw Unicode inputs without any normalization steps. Let’s now take a look at the\n",
        "pretokenization:"
      ],
      "metadata": {
        "id": "w7GWqNJJ7vZu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6pQX_3H0UTU",
        "outputId": "c3d24039-0ab6-45a9-942e-1babf08f58d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('def', (0, 3)), ('Ġsay', (3, 7)), ('_', (7, 8)), ('hello', (8, 13)), ('():',\n",
            "(13, 16)), ('ĊĠĠĠ', (16, 20)), ('Ġprint', (20, 26)), ('(\"', (26, 28)), ('Hello',\n",
            "(28, 33)), (',', (33, 34)), ('ĠWorld', (34, 40)), ('!\")', (40, 43)), ('Ġ#', (43,\n",
            "45)), ('ĠPrint', (45, 51)), ('Ġit', (51, 54)), ('Ċ', (54, 55)), ('Ċ', (55, 56)),\n",
            "('say', (56, 59)), ('_', (59, 60)), ('hello', (60, 65)), ('()', (65, 67)), ('Ċ',\n",
            "(67, 68))]\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s start with the numbers. Tokenizers has a very useful feature for switching\n",
        "between strings and tokens, called offset tracking. All the operations on the input\n",
        "string are tracked so that it’s possible to know exactly what part of the input string a\n",
        "token after tokenization corresponds to. These numbers simply indicate where in the\n",
        "original string each token comes from; for instance, the word 'hello' in the first line\n",
        "corresponds to the characters 8 to 13 in the original string. If some characters are\n",
        "removed in a normalization step, we are thus still able to associate each token with\n",
        "the respective part in the original string.\n",
        "\n",
        "\n",
        "The other curious feature of the tokenized text is the odd-looking characters, such as\n",
        "Ċ and Ġ. Byte-level means that this tokenizer works on bytes instead of Unicode char‐\n",
        "acters. Each Unicode character is composed of between 1 and 4 bytes, depending on\n",
        "the character. The nice thing about bytes is that while there are 143,859 Unicode\n",
        "characters in the Unicode alphabet, there are only 256 elements in the byte alphabet,and you can express each Unicode character as a sequence of these bytes. If we work\n",
        "on bytes we can thus express all the strings composed from the UTF-8 world as\n",
        "longer strings in this alphabet of 256 values. That is, we can have a model using an\n",
        "alphabet of only 256 words and be able to process any Unicode string. Let’s have a\n",
        "look at what the byte representations of some characters look like:"
      ],
      "metadata": {
        "id": "-r_Ddx7m72bR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oF_aUIIX0UTV",
        "outputId": "136de6ef-b639-4b0c-9b04-44db5fd8c185"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "`a` is encoded as `b'a'` with a single byte: 97\n",
            "`€` is encoded as `b'\\xe2\\x82\\xac'` with three bytes: [226, 130, 172]\n"
          ]
        }
      ],
      "source": [
        "a, e = u\"a\", u\"€\"\n",
        "byte = ord(a.encode(\"utf-8\"))\n",
        "print(f'`{a}` is encoded as `{a.encode(\"utf-8\")}` with a single byte: {byte}')\n",
        "byte = [ord(chr(i)) for i in e.encode(\"utf-8\")]\n",
        "print(f'`{e}` is encoded as `{e.encode(\"utf-8\")}` with three bytes: {byte}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point you might wonder: why work on a byte level? Think back to our discus‐\n",
        "sion in Chapter 2 about the trade-offs between character and word tokens. We could\n",
        "decide to build our vocabulary from the 143,859 Unicode characters, but we would\n",
        "also like to include words—i.e., combinations of Unicode characters—in our vocabu‐\n",
        "lary, so this (already very large) size is only a lower bound for the total size of the\n",
        "vocabulary. This will make our model’s embedding layer very large because it compri‐\n",
        "ses one vector for each vocabulary token.\n",
        "\n",
        "On the other extreme, if we only use the 256 byte values as our vocabulary, the input\n",
        "sequences will be segmented in many small pieces (each byte constituting the Uni‐\n",
        "code characters), and as such our model will have to work on long inputs and spend\n",
        "significant compute power on reconstructing Unicode characters from their separate\n",
        "bytes, and then words from these characters. See the paper accompanying the ByT5\n",
        "model release for a detailed study of this overhead.6\n",
        "\n",
        "\n",
        "A middle-ground solution is to construct a medium-sized vocabulary by extending\n",
        "the 256-word vocabulary with the most common combinations of bytes. This is the\n",
        "approach taken by the BPE algorithm. The idea is to progressively construct a\n",
        "vocabulary of a predefined size by creating new vocabulary tokens through iteratively\n",
        "merging the most frequently co-occurring pair of tokens in the vocabulary. For\n",
        "instance, if t and h occur very frequently together, like in English, we’ll add a token th\n",
        "to the vocabulary to model this pair of tokens instead of keeping them separated. The\n",
        "t and h tokens are kept in the vocabulary to tokenize instances where they do not\n",
        "occur together. Starting from a basic vocabulary of elementary units, we can then\n",
        "model any string efficiently."
      ],
      "metadata": {
        "id": "3GG56cF28Pkj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is just one issue when using a typical BPE algorithm in NLP. These algorithms\n",
        "are designed to work with clean Unicode string as inputs, not bytes, and expect regu‐\n",
        "lar ASCII characters in the inputs, without spaces or control characters. But in the\n",
        "Unicode characters corresponding to the 256 first bytes, there are many control char‐\n",
        "acters (newline, tab, escape, line feed, and other nonprintable characters). To over‐\n",
        "come this problem, the GPT-2 tokenizer first maps all the 256 input bytes to Unicode\n",
        "strings that can easily be digested by the standard BPE algorithms—that is, we will\n",
        "map our 256 elementary values to Unicode strings that all correspond to standard\n",
        "printable Unicode characters."
      ],
      "metadata": {
        "id": "-U0VkHus81yq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjdPKuMI0UTW",
        "outputId": "7ba29979-4c77-4120-9b2e-0431e59b9130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of our base vocabulary: 256\n",
            "First element: `!`, last element: `Ń`\n"
          ]
        }
      ],
      "source": [
        "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
        "\n",
        "byte_to_unicode_map = bytes_to_unicode()\n",
        "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
        "base_vocab = list(unicode_to_byte_map.keys())\n",
        "\n",
        "print(f'Size of our base vocabulary: {len(base_vocab)}')\n",
        "print(f'First element: `{base_vocab[0]}`, last element: `{base_vocab[-1]}`')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNRSDoJi0UTW",
        "outputId": "fb458770-a88b-49d5-a5c1-fc711410991f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Description</th>\n",
              "      <th>Character</th>\n",
              "      <th>Bytes</th>\n",
              "      <th>Mapped bytes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>Regular characters</td>\n",
              "      <td>`a` and `?`</td>\n",
              "      <td>97 and 63</td>\n",
              "      <td>`a` and `?`</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Non-printable control character (CARRIAGE RETURN)</td>\n",
              "      <td>`U+000D`</td>\n",
              "      <td>13</td>\n",
              "      <td>`č`</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>A space</td>\n",
              "      <td>` `</td>\n",
              "      <td>32</td>\n",
              "      <td>`Ġ`</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>A non-breakable space</td>\n",
              "      <td>`\\xa0`</td>\n",
              "      <td>160</td>\n",
              "      <td>`ł`</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>A newline character</td>\n",
              "      <td>`\\n`</td>\n",
              "      <td>10</td>\n",
              "      <td>`Ċ`</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# hide_input\n",
        "#id unicode_mapping\n",
        "#caption Examples of character mappings in BPE\n",
        "#hide_input\n",
        "import pandas as pd\n",
        "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
        "\n",
        "byte_to_unicode_map = bytes_to_unicode()\n",
        "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
        "base_vocab = list(unicode_to_byte_map.keys())\n",
        "\n",
        "examples = [\n",
        "    ['Regular characters', '`a` and `?`', f'{ord(\"a\")} and {ord(\"?\")}' , f'`{byte_to_unicode_map[ord(\"a\")]}` and `{byte_to_unicode_map[ord(\"?\")]}`'],\n",
        "    ['Nonprintable control character (carriage return)', '`U+000D`', f'13', f'`{byte_to_unicode_map[13]}`'],\n",
        "    ['A space', '` `', f'{ord(\" \")}', f'`{byte_to_unicode_map[ord(\" \")]}`'],\n",
        "    ['A nonbreakable space', '`\\\\xa0`', '160', f'`{byte_to_unicode_map[ord(chr(160))]}`'],\n",
        "    ['A newline character', '`\\\\n`', '10', f'`{byte_to_unicode_map[ord(chr(10))]}`'],\n",
        "]\n",
        "\n",
        "pd.DataFrame(examples, columns = ['Description', 'Character', 'Bytes', 'Mapped bytes'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could have used a more explicit conversion, like mapping newlines to a NEWLINE\n",
        "string, but BPE algorithms are typically designed to work on characters. For this rea‐\n",
        "son, keeping one Unicode character for each byte character is easier to handle with an\n",
        "out-of-the-box BPE algorithm. Now that we have been introduced to the dark magic\n",
        "of Unicode encodings, we can understand our tokenization conversion a bit better:"
      ],
      "metadata": {
        "id": "mbmR36K49Aaq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1bGGBua0UTW",
        "outputId": "a8f09145-8e0c-4e79-c09c-c8f36c5f1fec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('def', (0, 3)), ('Ġsay', (3, 7)), ('_', (7, 8)), ('hello', (8, 13)), ('():',\n",
            "(13, 16)), ('ĊĠĠĠ', (16, 20)), ('Ġprint', (20, 26)), ('(\"', (26, 28)), ('Hello',\n",
            "(28, 33)), (',', (33, 34)), ('ĠWorld', (34, 40)), ('!\")', (40, 43)), ('Ġ#', (43,\n",
            "45)), ('ĠPrint', (45, 51)), ('Ġit', (51, 54)), ('Ċ', (54, 55)), ('Ċ', (55, 56)),\n",
            "('say', (56, 59)), ('_', (59, 60)), ('hello', (60, 65)), ('()', (65, 67)), ('Ċ',\n",
            "(67, 68))]\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can recognize the newlines, which as we now know are mapped to Ċ, and the\n",
        "spaces, mapped to Ġ. We also see that:\n",
        "• Spaces, and in particular consecutive spaces, are conserved (for instance, the\n",
        "three spaces in 'ĊĠĠĠ').\n",
        "• Consecutive spaces are considered as a single word.\n",
        "• Each space preceding a word is attached to and considered a part of the subse‐\n",
        "quent word (e.g., in 'Ġsay')."
      ],
      "metadata": {
        "id": "yHCcg1jx9BXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s now experiment with the BPE model. As we’ve mentioned, it’s in charge of split‐\n",
        "ting the words into subunits until all subunits belong to the predefined vocabulary.\n",
        "The vocabulary of our GPT-2 tokenizer comprises 50,257 words:\n",
        "• The base vocabulary with the 256 values of the bytes\n",
        "\n",
        "• 50,000 additional tokens created by repeatedly merging the most commonly co-\n",
        "occurring tokens\n",
        "\n",
        "• A special character added to the vocabulary to represent document boundaries\n",
        "We can easily check that by looking at the length attribute of the tokenizer:"
      ],
      "metadata": {
        "id": "fI3hBO9s9MFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hwVMx7Gp8-Ej"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulF3ztMd0UTW",
        "outputId": "9f1e64dd-af23-41c2-d19a-2b0432cb1810"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of the vocabulary: 50257\n"
          ]
        }
      ],
      "source": [
        "print(f\"Size of the vocabulary: {len(tokenizer)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfOD20UY0UTW",
        "outputId": "17ab9543-ffd5-4250-eb6b-f77bc6ce4dec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['def', 'Ġsay', '_', 'hello', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"',\n",
            "'Hello', ',', 'ĠWorld', '!\"', ')', 'Ġ#', 'ĠPrint', 'Ġit', 'Ċ', 'Ċ', 'say', '_',\n",
            "'hello', '()', 'Ċ']\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer(python_code).tokens())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the BPE tokenizer keeps most of the words but will split the multiple\n",
        "spaces of our indentation into several consecutive spaces. This happens because this\n",
        "tokenizer is not specifically trained on code, but mostly on texts where consecutive\n",
        "spaces are rare. The BPE model thus doesn’t include a specific token in the vocabu‐\n",
        "lary for indentation. This is a case where the tokenizer model is poorly suited for the\n",
        "dataset’s domain. As we discussed earlier, the solution is to retrain the tokenizer on\n",
        "the target corpus. So let’s get to it!"
      ],
      "metadata": {
        "id": "i1Ra3om_9Rfp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNDvkWzD0UTW"
      },
      "source": [
        "### Training a Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s retrain our byte-level BPE tokenizer on a slice of our corpus to get a vocabulary\n",
        "better adapted to Python code. Retraining a tokenizer provided by Transformers is\n",
        "simple. We just need to:\n",
        "• Specify our target vocabulary size.\n",
        "• Prepare an iterator to supply lists of input strings to process to train the tokeniz‐\n",
        "er’s model.\n",
        "• Call the train_new_from_iterator() method."
      ],
      "metadata": {
        "id": "RlmKConG9WVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike deep learning models, which are often expected to memorize a lot of specific\n",
        "details from the training corpus, tokenizers are really just trained to extract the main\n",
        "statistics. In a nutshell, the tokenizer is just trained to know which letter combina‐\n",
        "tions are the most frequent in our corpus.\n",
        "Therefore, you don’t necessarily need to train your tokenizer on a very large corpus;\n",
        "the corpus just needs to be representative of your domain and big enough for the\n",
        "tokenizer to extract statistically significant measures. But depending on the vocabu‐\n",
        "lary size and the exact texts in the corpus, the tokenizer can end up storing\n",
        "unexpected words. We can see this, for instance, when looking at the longest words in\n",
        "the vocabulary of the GPT-2 tokenizer:"
      ],
      "metadata": {
        "id": "scHmYXGj9akm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60dQusy20UTX",
        "outputId": "060ec1b3-7898-46da-e9dc-ad271d45c32e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ', '\n",
            "=================================================================', '\n",
            "----------------------------------------------------------------',\n",
            "'................................................................',\n",
            "'ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ',\n",
            "'----------------------------------------------------------------',\n",
            "'================================================================',\n",
            "'________________________________________________________________']\n"
          ]
        }
      ],
      "source": [
        "tokens = sorted(tokenizer.vocab.items(), key=lambda x: len(x[0]), reverse=True)\n",
        "print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[:8]]);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These tokens look like separator lines that are likely to be used on forums. This\n",
        "makes sense since GPT-2 was trained on a corpus centered around Reddit. Now let’s\n",
        "have a look at the last words that were added to the vocabulary, and thus the least\n",
        "frequent ones:"
      ],
      "metadata": {
        "id": "zfZGva7d9eAK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdawy6GW0UTX",
        "outputId": "7f7d153b-cf54-4645-800b-cec720256568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<|endoftext|>', ' gazed', ' informants', ' Collider', ' regress', 'ominated',\n",
            "' amplification', 'Compar', '….\"', ' (/', 'Commission', ' Hitman']\n"
          ]
        }
      ],
      "source": [
        "tokens = sorted(tokenizer.vocab.items(), key=lambda x: x[1], reverse=True)\n",
        "print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[:12]]);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first token, <|endoftext|>, is the special token used to specify the end of a text\n",
        "sequence and was added after the BPE vocabulary was built. For each of these tokens\n",
        "our model will have to learn an associated word embedding, and we probably don’t\n",
        "want the embedding matrix to contain too many noisy words.\n",
        "\n",
        "\n",
        "Also note how some\n",
        "very time- and space-specific knowledge of the world (e.g., proper nouns like Hitman\n",
        "and Commission) is embedded at a very low level in our modeling approach by these\n",
        "words being granted separate tokens with associated vectors in the vocabulary. The\n",
        "creation of such specific tokens by a BPE tokenizer can also be an indication that the\n",
        "target vocabulary size is too large or that the corpus contains idiosyncratic tokens."
      ],
      "metadata": {
        "id": "3cjKoITKx6WS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s train a fresh tokenizer on our corpus and examine its learned vocabulary. Since\n",
        "we just need a corpus reasonably representative of our dataset statistics, let’s select\n",
        "about 1–2 GB of data, or about 100,000 documents from our corpus:"
      ],
      "metadata": {
        "id": "QPF7pnZNyF6a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "743bca69d71649908db9ca5760af61d2"
          ]
        },
        "id": "-fDeO0bg0UTX",
        "outputId": "a394fc8a-e354-4e46-a599-748791e67b30"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "743bca69d71649908db9ca5760af61d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Check remote data files:   0%|          | 0/183 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration codeparrot-train-99775fd6743284b5\n"
          ]
        }
      ],
      "source": [
        "#hide_output\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "length = 100000\n",
        "dataset_name = 'transformersbook/codeparrot-train'\n",
        "dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
        "iter_dataset = iter(dataset)\n",
        "\n",
        "def batch_iterator(batch_size=10):\n",
        "    for _ in tqdm(range(0, length, batch_size)):\n",
        "        yield [next(iter_dataset)['content'] for _ in range(batch_size)]\n",
        "\n",
        "new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(),\n",
        "                                                  vocab_size=12500,\n",
        "                                                  initial_alphabet=base_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s investigate the first and last words created by our BPE algorithm to see how rele‐\n",
        "vant our vocabulary is. We skip the 256 byte tokens and look at the first tokens added\n",
        "thereafter:\n",
        "\n"
      ],
      "metadata": {
        "id": "mCy3ZTMDyRak"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek25me8g0UTX",
        "outputId": "76223de0-5b96-4368-ecb8-a696e1006792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['  ', '    ', '   ', '        ', 'se', 'in', '       ', 're', 'on', 'te', '\\n\n",
            "', '\\n        ', 'or', 'st', 'de', '\\n   ', 'th', 'le', ' =', 'lf', 'self',\n",
            "'me', 'al']\n"
          ]
        }
      ],
      "source": [
        "tokens = sorted(new_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\n",
        "print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[257:280]]);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see various standard levels of indentation and whitespace tokens, as well\n",
        "as short common Python keywords like self, or, and in. This is a good sign that our\n",
        "BPE algorithm is working as intended. Now let’s check out the last words:"
      ],
      "metadata": {
        "id": "jvEWJc0IyWDi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLo3107P0UTY",
        "outputId": "d2624eb2-fd88-448c-9a8c-f51c4892850f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[' capt', ' embedded', ' regarding', 'Bundle', '355', ' recv', ' dmp', ' vault',\n",
            "' Mongo', ' possibly', 'implementation', 'Matches']\n"
          ]
        }
      ],
      "source": [
        "print([f'{new_tokenizer.convert_tokens_to_string(t)}' for t,_ in tokens[-12:]]);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here there are still some relatively common words, like recv, as well as some more\n",
        "noisy words probably coming from the comments.\n",
        "We can also tokenize our simple example of Python code to see how our tokenizer is\n",
        "behaving on a simple example:"
      ],
      "metadata": {
        "id": "_yoCseomyedF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRUZYcsa0UTY",
        "outputId": "31cda0eb-708b-4f1a-8925-07e45961aafa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['def', 'Ġs', 'ay', '_', 'hello', '():', 'ĊĠĠĠ', 'Ġprint', '(\"', 'Hello', ',',\n",
            "'ĠWor', 'ld', '!\")', 'Ġ#', 'ĠPrint', 'Ġit', 'Ċ', 'Ċ', 's', 'ay', '_', 'hello',\n",
            "'()', 'Ċ']\n"
          ]
        }
      ],
      "source": [
        "print(new_tokenizer(python_code).tokens())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though they are not code keywords, it’s a little annoying to see common English\n",
        "words like World or say being split by our tokenizer, since we’d expect them to occur\n",
        "rather frequently in the corpus. Let’s check if all the Python reserved keywords are in\n",
        "the vocabulary:"
      ],
      "metadata": {
        "id": "I_td1iowyjdd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziYLnLmi0UTY",
        "outputId": "6bf75d26-4e46-4543-f523-b95ab66e6314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are in total 35 Python keywords.\n",
            "No, keyword `await` is not in the vocabulary\n",
            "No, keyword `finally` is not in the vocabulary\n",
            "No, keyword `nonlocal` is not in the vocabulary\n"
          ]
        }
      ],
      "source": [
        "import keyword\n",
        "\n",
        "print(f'There are in total {len(keyword.kwlist)} Python keywords.')\n",
        "for keyw in keyword.kwlist:\n",
        "    if keyw not in new_tokenizer.vocab:\n",
        "        print(f'No, keyword `{keyw}` is not in the vocabulary')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that several quite frequent keywords, like finally, are not in the vocabu‐\n",
        "lary either. Let’s try building a larger vocabulary using a larger sample of our dataset.\n",
        "For instance, we can build a vocabulary of 32,768 words (multiples of 8 are better for\n",
        "some efficient GPU/TPU computations) and train the tokenizer on a twice as large\n",
        "slice of our corpus:"
      ],
      "metadata": {
        "id": "ir3p-bMIy1uZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvciI6030UTY",
        "outputId": "20e34d7e-b7ee-4f7e-db47-d81656641937"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [05:08<00:00,  1.54s/it]\n"
          ]
        }
      ],
      "source": [
        "# hide_output\n",
        "length = 200000\n",
        "new_tokenizer_larger = tokenizer.train_new_from_iterator(batch_iterator(),\n",
        "    vocab_size=32768, initial_alphabet=base_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slRQweOc0UTY",
        "outputId": "4fcc713b-39ad-4019-c315-56ce4c6a8ee7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['lineEdit', 'spik', ' BC', 'pective', 'OTA', 'theus', 'FLUSH', ' excutils',\n",
            "'00000002', ' DIVISION', 'CursorPosition', ' InfoBar']\n"
          ]
        }
      ],
      "source": [
        "tokens = sorted(new_tokenizer_larger.vocab.items(), key=lambda x: x[1],\n",
        "                reverse=False)\n",
        "print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[-12:]]);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A brief inspection doesn’t show any regular programming keywords here, which is\n",
        "promising. Let’s try tokenizing our sample code example with the new larger\n",
        "tokenizer:"
      ],
      "metadata": {
        "id": "DceZS5JUy640"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0r7oSARI0UTZ",
        "outputId": "24f0e3f0-c313-4b14-c266-3f26f7258679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['def', 'Ġsay', '_', 'hello', '():', 'ĊĠĠĠ', 'Ġprint', '(\"', 'Hello', ',',\n",
            "'ĠWorld', '!\")', 'Ġ#', 'ĠPrint', 'Ġit', 'Ċ', 'Ċ', 'say', '_', 'hello', '()',\n",
            "'Ċ']\n"
          ]
        }
      ],
      "source": [
        "print(new_tokenizer_larger(python_code).tokens())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here also the indents are conveniently kept in the vocabulary, and we see that com‐\n",
        "mon English words like Hello, World, and say are also included as single tokens. This\n",
        "seems more in line with our expectations of the data the model may see in the down‐\n",
        "stream task. Let’s investigate the common Python keywords, as we did before:"
      ],
      "metadata": {
        "id": "tocF1O8py_jz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmYCLvPC0UTZ",
        "outputId": "89465d0c-cae1-4e0b-9713-bb8e23090f7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No, keyword `nonlocal` is not in the vocabulary\n"
          ]
        }
      ],
      "source": [
        "for keyw in keyword.kwlist:\n",
        "    if keyw not in new_tokenizer_larger.vocab:\n",
        "        print(f'No, keyword `{keyw}` is not in the vocabulary')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are still missing the nonlocal keyword, but it’s also rarely used in practice as it\n",
        "makes the syntax more complex. Keeping it out of the vocabulary seems reasonable."
      ],
      "metadata": {
        "id": "-u46GMKAzDh6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZPELDR_0UTZ"
      },
      "source": [
        "### Saving a Custom Tokenizer on the Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our tokenizer is trained, we should save it. The simplest way to save it and\n",
        "be able to access it from anywhere later is to push it to the Hugging Face Hub. This\n",
        "will be especially useful later, when we use a separate training server."
      ],
      "metadata": {
        "id": "JTwZ5S9BzG9R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXKC1DFE0UTZ",
        "outputId": "128bc357-cb28-435e-fa72-0e6df46759a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning https://huggingface.co/transformersbook/codeparrot into local empty directory.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/transformersbook/codeparrot/commit/1c284adaa3cc9f8635ae7e3377bd3739f48bc09a'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#hide_output\n",
        "model_ckpt = \"codeparrot\"\n",
        "org = \"transformersbook\"\n",
        "new_tokenizer_larger.push_to_hub(model_ckpt, organization=org)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you don’t want to push to an organization, you can simply omit the organization\n",
        "argument. This will create a repository in your namespace named codeparrot, which\n",
        "anyone can then load by running:"
      ],
      "metadata": {
        "id": "qz6RGkzMzMOC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lqSl-dh0UTZ",
        "outputId": "ffbb1025-b044-498f-b8b8-1686f8bb9fa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['def', 'Ġsay', '_', 'hello', '():', 'ĊĠĠĠ', 'Ġprint', '(\"', 'Hello', ',',\n",
            "'ĠWorld', '!\")', 'Ġ#', 'ĠPrint', 'Ġit', 'Ċ', 'Ċ', 'say', '_', 'hello', '()',\n",
            "'Ċ']\n"
          ]
        }
      ],
      "source": [
        "reloaded_tokenizer = AutoTokenizer.from_pretrained(org + \"/\" + model_ckpt)\n",
        "print(reloaded_tokenizer(python_code).tokens())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer loaded from the Hub behaves exactly as we just saw. We can also inves‐\n",
        "tigate its files and saved vocabulary on the Hub. For reproducibility, let’s save our\n",
        "smaller tokenizer as well:"
      ],
      "metadata": {
        "id": "rk4n3T6wzPC9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5owS9GWi0UTZ",
        "outputId": "8649b4a2-6c3e-4187-eb2c-dd57763024ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning https://huggingface.co/transformersbook/codeparrot-small-vocabulary into local empty directory.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/transformersbook/codeparrot-small-vocabulary/commit/0b37bed9956d95d0b79ada169f6a281e15c63381'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#hide_output\n",
        "new_tokenizer.push_to_hub(model_ckpt+ \"-small-vocabulary\", organization=org)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtrjqiJi0UTZ"
      },
      "source": [
        "## Training a Model from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s the part you’ve probably been waiting for: the model training. In this section\n",
        "we’ll decide which architecture works best for the task, initialize a fresh model\n",
        "without pretrained weights, set up a custom data loading class, and create a scalable\n",
        "training loop. In the grand finale we will train small and large GPT-2 models with 111\n",
        "million and 1.5 billion parameters, respectively!"
      ],
      "metadata": {
        "id": "GhetR0GFzVNH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J96RG5B10UTa"
      },
      "source": [
        "### A Tale of Pretraining Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With such a large\n",
        "codebase consisting of code snippets like the one shown in Figure 10-1, we can tackle\n",
        "several tasks. Which one we choose will influence our choice of pretraining objec‐\n",
        "tives. Let’s have a look at three common tasks."
      ],
      "metadata": {
        "id": "HhYbUYK1zYhh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neB8-vv70UTa"
      },
      "source": [
        "#### Causal language modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A natural task with textual data is to provide a model with the beginning of a code\n",
        "sample and ask it to generate possible completions. This is a self-supervised training\n",
        "objective in which we can use the dataset without annotations. This should ring a\n",
        "bell: it’s the causal language modeling task we encountered in Chapter 5. A directly\n",
        "related downstream task is code autocompletion, so we’ll definitely put this model on\n",
        "the shortlist. A decoder-only architecture such as the GPT family of models is usually\n",
        "best suited for this task, as shown in Figure 10-2."
      ],
      "metadata": {
        "id": "KQk2ZRZ9zhiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure 10-2. In causal language modeling, the future tokens are masked and the model\n",
        "has to predict them; typically a decoder model such as GPT is used for such a task"
      ],
      "metadata": {
        "id": "Aw5DwZmWzl_N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIqV1tNA0UTa"
      },
      "source": [
        "#### Masked language modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A related but slightly different task is to provide a model with a noisy code sample,\n",
        "for instance with a code instruction replaced by a random or masked word, and ask it\n",
        "to reconstruct the original clean sample, as illustrated in Figure 10-3. This is also a\n",
        "self-supervised training objective and is commonly called masked language modeling\n",
        "or the denoising objective.\n",
        "\n",
        "\n",
        "It’s harder to think about a downstream task directly related\n",
        "to denoising, but denoising is generally a good pretraining task to learn general rep‐\n",
        "resentations for later downstream tasks.\n",
        "\n",
        "\n",
        "Many of the models that we have used in the\n",
        "previous chapters (like BERT and XLM-RoBERTa) are pretrained in that way. Train‐\n",
        "\n",
        "ing a masked language model on a large corpus can thus be combined with fine-\n",
        "tuning the model on a downstream task with a limited number of labeled examples.\n",
        "\n",
        "\n",
        "Figure 10-3. In masked language modeling some of the input tokens are either masked or\n",
        "replaced, and the model’s task is to predict the original tokens; this is the architecture\n",
        "underlying the encoder branch of transformer models"
      ],
      "metadata": {
        "id": "eI33RbTbzpKM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoMuRIe00UTa"
      },
      "source": [
        "<img alt=\"MLM pretraining\" caption=\"In masked language modeling some of the input tokens are either masked or replaced, and the model's task is to predict the original tokens; this is the architecture underlying the encoder branch of transformer models\" src=\"images/chapter10_pretraining-mlm.png\" id=\"pretraining-mlm\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1M-Z65k0UTa"
      },
      "source": [
        "#### Sequence-to-sequence training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An alternative task is to use a heuristic like regular expressions to separate comments\n",
        "or docstrings from code and build a large-scale dataset of (code, comments) pairs that\n",
        "can be used as an annotated dataset.\n",
        "\n",
        "The training task is then a supervised training\n",
        "objective in which one category (code or comment) is used as input for the model\n",
        "and the other category (comment or code) is used as labels. This is a case of super‐\n",
        "vised learning with (input, labels) pairs, as highlighted in Figure 10-4.\n",
        "\n",
        "\n",
        "With a large,\n",
        "clean, and diverse dataset as well as a model with sufficient capacity, we can try to\n",
        "train a model that learns to transcript comments in code or vice versa. A downstream\n",
        "task directly related to this supervised training task is then documentation generation\n",
        "from code or code generation from documentation,"
      ],
      "metadata": {
        "id": "yWkpheOlz6bo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this setting a sequence is translated into another sequence, which is\n",
        "where encoder-decoder architectures such as T5, BART, and PEGASUS shine.\n",
        "\n",
        "\n",
        "Since we want to build a code autocompletion model, we’ll select the first objective\n",
        "and choose a GPT architecture for the task. So let’s initialize a fresh GPT-2 model!"
      ],
      "metadata": {
        "id": "viYkWq5z0DHc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQqQQyUC0UTb"
      },
      "source": [
        "<img alt=\"Seq2seq pretraining\" caption=\"Using an encoder-decoder architecture for a sequence-to-sequence task where the inputs are split into comment/code pairs using heuristics: the model gets one element as input and needs to generate the other one\" src=\"images/chapter10_pretraining-seq2seq.png\" id=\"pretraining-seq2seq\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "622lpJ3C0UTb"
      },
      "source": [
        "### Initializing the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the first time in this book that we won’t use the from_pretrained() method to\n",
        "load a model but initialize the new model. We will, however, load the configuration of\n",
        "gpt2-xl so that we use the same hyperparameters and only adapt the vocabulary size\n",
        "for the new tokenizer."
      ],
      "metadata": {
        "id": "hdOTAGf50Or9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeJtX1XW0UTb"
      },
      "source": [
        "> **NOTE**: In the following code block, a large GPT-2 checkpoint is loaded into memory. On platforms like Colab and Kaggle, this can cause the instance to crash due to insufficient RAM or GPU memory. You can still run the example if you use the small checkpoint by replacing the configuration with `config = AutoConfig.from_pretrained(\"gpt2\", vocab_size=len(tokenizer))`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "be84ca77ca144954af8ae4820ec6685b"
          ]
        },
        "id": "Wwj1vvNl0UTb",
        "outputId": "3ff1dbe1-dc6d-41da-cefa-a2c0a97faa9e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be84ca77ca144954af8ae4820ec6685b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/787 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#hide_output\n",
        "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(org + \"/\" + model_ckpt)\n",
        "config = AutoConfig.from_pretrained(\"gpt2-xl\", vocab_size=len(tokenizer))\n",
        "model = AutoModelForCausalLM.from_config(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd4kKRjE0UTb",
        "outputId": "e7f6179a-6dcd-40d2-fea1-82efb48d09a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT-2 (xl) size: 1529.6M parameters\n"
          ]
        }
      ],
      "source": [
        "print(f'GPT-2 (xl) size: {model_size(model)/1000**2:.1f}M parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a 1.5B parameter model! This is a lot of capacity, but we also have a large data‐\n",
        "set. In general, large language models are more efficient to train as long as the dataset\n",
        "is reasonably large. Let’s save the newly initialized model in a models/ folder and push\n",
        "it to the Hub:"
      ],
      "metadata": {
        "id": "IhyBKfJl0Ya5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3puMtl1u0UTc"
      },
      "outputs": [],
      "source": [
        "#hide_output\n",
        "model.save_pretrained(\"models/\" + model_ckpt, push_to_hub=True,\n",
        "                      organization=org)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this model is quite large, we’ll also create a smaller version that\n",
        "we can train to make sure everything works before scaling up. We will take the stan‐\n",
        "dard GPT-2 size as a base:"
      ],
      "metadata": {
        "id": "CVO6TTLk0dYb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsL7JP440UTc"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "config_small = AutoConfig.from_pretrained(\"gpt2\", vocab_size=len(tokenizer))\n",
        "model_small = AutoModelForCausalLM.from_config(config_small)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cg7KWTZl0UTd",
        "outputId": "4dd62cea-4487-4be5-8ab4-064977a8ea4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT-2 size: 111.0M parameters\n"
          ]
        }
      ],
      "source": [
        "print(f'GPT-2 size: {model_size(model_small)/1000**2:.1f}M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSIZ5sm10UTd",
        "outputId": "073b8f58-2d9f-482b-9e9d-1a8afd8b5d89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning https://huggingface.co/transformersbook/codeparrot-small into local empty directory.\n"
          ]
        }
      ],
      "source": [
        "#hide_output\n",
        "model_small.save_pretrained(\"models/\" + model_ckpt + \"-small\", push_to_hub=True,\n",
        "                            organization=org)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have two models we can train, we need to make sure we can feed them\n",
        "the input data efficiently during training."
      ],
      "metadata": {
        "id": "HEUCie0k0hPc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_UQ3XcV0UTd"
      },
      "source": [
        "### Implementing the Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be able to train with maximal efficiency, we will want to supply our model with\n",
        "sequences filling its context. For example, if the context length of our model is 1,024\n",
        "tokens, we always want to provide 1,024-token sequences during training. But some\n",
        "of our code examples might be shorter or longer than 1,024 tokens. To feed batches\n",
        "with full sequences of sequence_length to our model, we should thus either drop the\n",
        "last incomplete sequence or pad it."
      ],
      "metadata": {
        "id": "I7tfK1Km0lUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, this will render our training slightly less\n",
        "efficient and force us to take care of padding and masking padded token labels. We\n",
        "are much more compute- than data-constrained, so we’ll take the easy and efficient\n",
        "way here. We can use a little trick to make sure we don’t lose too many trailing seg‐\n",
        "ments: we can tokenize several examples and then concatenate them, separated by the\n",
        "special end-of-sequence token, to get a very long sequence."
      ],
      "metadata": {
        "id": "TczkYt_c0ql1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we split this\n",
        "sequence into equally sized chunks as shown in Figure 10-5. With this approach, we\n",
        "lose at most a small fraction of the data at the end."
      ],
      "metadata": {
        "id": "hvH7aljH0s4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can, for instance, make sure we have roughly one hundred full sequences in our\n",
        "tokenized examples by defining our input string character length as:\n",
        "\n",
        "`input_characters = number_of_sequences * sequence_length * characters_per_token`\n",
        "\n",
        "where:\n",
        "• input_characters is the number of characters in the string input to our\n",
        "tokenizer.\n",
        "• number_of_sequences is the number of (truncated) sequences we would like\n",
        "from our tokenizer, (e.g., 100).\n",
        "• sequence_length is the number of tokens per sequence returned by the token‐\n",
        "izer, (e.g., 1,024).\n",
        "• characters_per_token is the average number of characters per output token\n",
        "that we first need to estimate."
      ],
      "metadata": {
        "id": "scWKVZsi068A"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwxCdk4C0UTd"
      },
      "source": [
        "<img alt=\"Preprocessing for CLM\" caption=\"Preparing sequences of varying length for causal language modeling by concatenating several tokenized examples with an EOS token  before chunking them\" src=\"images/chapter10_preprocessing-clm.png\" id=\"preprocessing-clm\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s first estimate the average character length per token in our dataset:"
      ],
      "metadata": {
        "id": "fBX5nVEm1Fj3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djcm9rOg0UTd",
        "outputId": "25fe759f-6e82-4578-cb12-fc175fa8ec5a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/500 [00:00<01:16,  6.54it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2605 > 1024). Running this sequence through the model will result in indexing errors\n",
            "100%|██████████| 500/500 [00:04<00:00, 122.59it/s]\n"
          ]
        }
      ],
      "source": [
        "#hide_output\n",
        "examples, total_characters, total_tokens = 500, 0, 0\n",
        "dataset = load_dataset('transformersbook/codeparrot-train', split='train',\n",
        "                       streaming=True)\n",
        "\n",
        "for _, example in tqdm(zip(range(examples), iter(dataset)), total=examples):\n",
        "    total_characters += len(example['content'])\n",
        "    total_tokens += len(tokenizer(example['content']).tokens())\n",
        "\n",
        "characters_per_token = total_characters / total_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O39EusRf0UTe",
        "outputId": "7dd3cd78-5964-4c49-a4e8-7c4f00ad976e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.6233025034779565\n"
          ]
        }
      ],
      "source": [
        "print(characters_per_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With that we have all that’s needed to create our own IterableDataset (which is a\n",
        "helper class provided by PyTorch) for preparing constant-length inputs for the\n",
        "model. We just need to inherit from IterableDataset and set up the __iter__()\n",
        "function that yields the next element with the logic we just walked through:"
      ],
      "metadata": {
        "id": "GLf6zENA1fAw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--tns_1o0UTe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "class ConstantLengthDataset(IterableDataset):\n",
        "\n",
        "    def __init__(self, tokenizer, dataset, seq_length=1024,\n",
        "                 num_of_sequences=1024, chars_per_token=3.6):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.concat_token_id = tokenizer.eos_token_id\n",
        "        self.dataset = dataset\n",
        "        self.seq_length = seq_length\n",
        "        self.input_characters = seq_length * chars_per_token * num_of_sequences\n",
        "\n",
        "    def __iter__(self):\n",
        "        iterator = iter(self.dataset)\n",
        "        more_examples = True\n",
        "        while more_examples:\n",
        "            buffer, buffer_len = [], 0\n",
        "            while True:\n",
        "                if buffer_len >= self.input_characters:\n",
        "                    m=f\"Buffer full: {buffer_len}>={self.input_characters:.0f}\"\n",
        "                    print(m)\n",
        "                    break\n",
        "                try:\n",
        "                    m=f\"Fill buffer: {buffer_len}<{self.input_characters:.0f}\"\n",
        "                    print(m)\n",
        "                    buffer.append(next(iterator)[\"content\"])\n",
        "                    buffer_len += len(buffer[-1])\n",
        "                except StopIteration:\n",
        "                    iterator = iter(self.dataset)\n",
        "\n",
        "            all_token_ids = []\n",
        "            tokenized_inputs = self.tokenizer(buffer, truncation=False)\n",
        "            for tokenized_input in tokenized_inputs['input_ids']:\n",
        "                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
        "\n",
        "            for i in range(0, len(all_token_ids), self.seq_length):\n",
        "                input_ids = all_token_ids[i : i + self.seq_length]\n",
        "                if len(input_ids) == self.seq_length:\n",
        "                    yield torch.tensor(input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The __iter__() function builds up a buffer of strings until it contains enough char‐\n",
        "acters. All the elements in the buffer are tokenized and concatenated with the EOS\n",
        "token, then the long sequence in all_token_ids is chunked in seq_length-sized sli‐\n",
        "ces."
      ],
      "metadata": {
        "id": "v567iC501lkx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldiLad9X0UTe",
        "outputId": "97bde600-5b36-41b2-f0bb-5fc024d86484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fill buffer: 0<36864\n",
            "Fill buffer: 3311<36864\n",
            "Fill buffer: 9590<36864\n",
            "Fill buffer: 22177<36864\n",
            "Fill buffer: 25530<36864\n",
            "Fill buffer: 31098<36864\n",
            "Fill buffer: 32232<36864\n",
            "Fill buffer: 33867<36864\n",
            "Buffer full: 41172>=36864\n",
            "Lengths of the sequences: [1024, 1024, 1024, 1024, 1024]\n"
          ]
        }
      ],
      "source": [
        "shuffled_dataset = dataset.shuffle(buffer_size=100)\n",
        "constant_length_dataset = ConstantLengthDataset(tokenizer, shuffled_dataset,\n",
        "                                                num_of_sequences=10)\n",
        "dataset_iterator = iter(constant_length_dataset)\n",
        "\n",
        "lengths = [len(b) for _, b in zip(range(5), dataset_iterator)]\n",
        "print(f\"Lengths of the sequences: {lengths}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpCPVjgu0UTe"
      },
      "source": [
        "### Defining the Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have all the elements to write our training loop. One obvious limitation of\n",
        "training our own language model is the memory limits on the GPUs we will use. Even\n",
        "on a modern graphics card you can’t train a model at GPT-2 scale in reasonable time.\n",
        "In this tutorial we will implement data parallelism, which will help us utilize several\n",
        "GPUs for training. Fortunately, we can use Accelerate to make our code scalable."
      ],
      "metadata": {
        "id": "pG1DgRPi1xG7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aQ927MD0UTe"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "# Commented parameters correspond to the small model\n",
        "config = {\"train_batch_size\": 2, # 12\n",
        "          \"valid_batch_size\": 2, # 12\n",
        "          \"weight_decay\": 0.1,\n",
        "          \"shuffle_buffer\": 1000,\n",
        "          \"learning_rate\": 2e-4, # 5e-4\n",
        "          \"lr_scheduler_type\": \"cosine\",\n",
        "          \"num_warmup_steps\": 750, # 2000\n",
        "          \"gradient_accumulation_steps\": 16, # 1\n",
        "          \"max_train_steps\": 50000, # 150000\n",
        "          \"max_eval_steps\": -1,\n",
        "          \"seq_length\": 1024,\n",
        "          \"seed\": 1,\n",
        "          \"save_checkpoint_steps\": 50000} # 15000\n",
        "\n",
        "args = Namespace(**config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrUZXly80UTf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import logging\n",
        "import wandb\n",
        "\n",
        "def setup_logging(project_name):\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO, handlers=[\n",
        "        logging.FileHandler(f\"log/debug_{accelerator.process_index}.log\"),\n",
        "        logging.StreamHandler()])\n",
        "    if accelerator.is_main_process: # We only want to set up logging once\n",
        "        wandb.init(project=project_name, config=args)\n",
        "        run_name = wandb.run.name\n",
        "        tb_writer = SummaryWriter()\n",
        "        tb_writer.add_hparams(vars(args), {'0': 0})\n",
        "        logger.setLevel(logging.INFO)\n",
        "        datasets.utils.logging.set_verbosity_debug()\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "    else:\n",
        "        tb_writer = None\n",
        "        run_name = ''\n",
        "        logger.setLevel(logging.ERROR)\n",
        "        datasets.utils.logging.set_verbosity_error()\n",
        "        transformers.utils.logging.set_verbosity_error()\n",
        "    return logger, tb_writer, run_name"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each worker gets a unique accelerator.process_index, which we use with the File\n",
        "Handler to write the logs of each worker to an individual file.\n",
        "\n",
        "We’ll also define a function to log the metrics with TensorBoard and Weights & Bia‐\n",
        "ses. We again use the accelerator.is_main_process here to ensure that we only log\n",
        "the metrics once and not for each worker:"
      ],
      "metadata": {
        "id": "PbnEaHER2Ht7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fIdHUzM0UTf"
      },
      "outputs": [],
      "source": [
        "def log_metrics(step, metrics):\n",
        "    logger.info(f\"Step {step}: {metrics}\")\n",
        "    if accelerator.is_main_process:\n",
        "        wandb.log(metrics)\n",
        "        [tb_writer.add_scalar(k, v, step) for k, v in metrics.items()]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Next, let’s write a function that creates the dataloaders for the training and validation\n",
        "sets with our brand new ConstantLengthDataset class:"
      ],
      "metadata": {
        "id": "RYsYDt6v2Ofa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "328dc6d7d05c452e8d8e2cab5b4b9c4e"
          ]
        },
        "id": "n5kvxSZT0UTf",
        "outputId": "795967dd-04ae-4bbc-edce-727290bbc8c3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "328dc6d7d05c452e8d8e2cab5b4b9c4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Check remote data files:   0%|          | 0/183 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration codeparrot-train-938ce362e6f661b1\n",
            "Using custom data configuration codeparrot-valid-29167601d8e69487\n"
          ]
        }
      ],
      "source": [
        "#hide_output\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "def create_dataloaders(dataset_name):\n",
        "    train_data = load_dataset(dataset_name+'-train', split=\"train\",\n",
        "                              streaming=True)\n",
        "    train_data = train_data.shuffle(buffer_size=args.shuffle_buffer,\n",
        "                                    seed=args.seed)\n",
        "    valid_data = load_dataset(dataset_name+'-valid', split=\"validation\",\n",
        "                              streaming=True)\n",
        "\n",
        "    train_dataset = ConstantLengthDataset(tokenizer, train_data,\n",
        "                                          seq_length=args.seq_length)\n",
        "    valid_dataset = ConstantLengthDataset(tokenizer, valid_data,\n",
        "                                          seq_length=args.seq_length)\n",
        "\n",
        "    train_dataloader=DataLoader(train_dataset, batch_size=args.train_batch_size)\n",
        "    eval_dataloader=DataLoader(valid_dataset, batch_size=args.valid_batch_size)\n",
        "    return train_dataloader, eval_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another aspect we need to implement is optimization. We will set up the optimizer\n",
        "and learning rate schedule in the main loop, but we define a helper function here to\n",
        "differentiate the parameters that should receive weight decay. In general, biases and\n",
        "LayerNorm weights are not subject to weight decay:"
      ],
      "metadata": {
        "id": "WYVapnON2Yvf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOeiv7I10UTf"
      },
      "outputs": [],
      "source": [
        "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
        "    params_with_wd, params_without_wd = [], []\n",
        "    for n, p in model.named_parameters():\n",
        "        if any(nd in n for nd in no_decay):\n",
        "            params_without_wd.append(p)\n",
        "        else:\n",
        "            params_with_wd.append(p)\n",
        "    return [{'params': params_with_wd, 'weight_decay': args.weight_decay},\n",
        "            {'params': params_without_wd, 'weight_decay': 0.0}]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we want to evaluate the model on the validation set from time to time, so let’s\n",
        "add an evaluation function we can call that calculates the loss and perplexity on the\n",
        "evaluation set:"
      ],
      "metadata": {
        "id": "mGR7IxDG2h-9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guY4DcnM0UTf"
      },
      "outputs": [],
      "source": [
        "def evaluate():\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(batch, labels=batch)\n",
        "        loss = outputs.loss.repeat(args.valid_batch_size)\n",
        "        losses.append(accelerator.gather(loss))\n",
        "        if args.max_eval_steps > 0 and step >= args.max_eval_steps: break\n",
        "    loss = torch.mean(torch.cat(losses))\n",
        "    try:\n",
        "\t\tperplexity = torch.exp(loss)\n",
        "\texcept OverflowError:\n",
        "\t\tperplexity = torch.tensor(float(\"inf\"))\n",
        "    return loss.item(), perplexity.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The perplexity measures how well the model’s output probability distributions pre‐\n",
        "dict the targeted tokens. So a lower perplexity corresponds to a better performance.\n",
        "Note that we can compute the perplexity by exponentiating the cross-entropy loss\n",
        "which we get from the model’s output. Especially at the start of training when the loss\n",
        "is still high, it is possible to get a numerical overflow when calculating the perplexity.\n",
        "We catch this error and set the perplexity to infinity in these instances.\n",
        "\n",
        "\n",
        "With the Repository class from the\n",
        "huggingface_hub library you can programmatically access the repository and pull,\n",
        "branch, commit, or push. We’ll use this in our script to continuously push model\n",
        "checkpoints to the Hub during training.\n",
        "Now that we have all these helper functions in place, we are ready to write the heart\n",
        "of the training script:"
      ],
      "metadata": {
        "id": "Xd59nytV2s80"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asuGoxsa0UTf"
      },
      "outputs": [],
      "source": [
        "set_seed(args.seed)\n",
        "\n",
        "# Accelerator\n",
        "accelerator = Accelerator()\n",
        "samples_per_step = accelerator.state.num_processes * args.train_batch_size\n",
        "\n",
        "# Logging\n",
        "logger, tb_writer, run_name = setup_logging(project_name.split(\"/\")[1])\n",
        "logger.info(accelerator.state)\n",
        "\n",
        "# Load model and tokenizer\n",
        "if accelerator.is_main_process:\n",
        "    hf_repo = Repository(\"./\", clone_from=project_name, revision=run_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./\", gradient_checkpointing=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./\")\n",
        "\n",
        "# Load dataset and dataloader\n",
        "train_dataloader, eval_dataloader = create_dataloaders(dataset_name)\n",
        "\n",
        "# Prepare the optimizer and learning rate scheduler\n",
        "optimizer = AdamW(get_grouped_params(model), lr=args.learning_rate)\n",
        "lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer,\n",
        "                             num_warmup_steps=args.num_warmup_steps,\n",
        "                             num_training_steps=args.max_train_steps,)\n",
        "def get_lr():\n",
        "    return optimizer.param_groups[0]['lr']\n",
        "\n",
        "# Prepare everything with our `accelerator` (order of args is not important)\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader)\n",
        "\n",
        "# Train model\n",
        "model.train()\n",
        "completed_steps = 0\n",
        "for step, batch in enumerate(train_dataloader, start=1):\n",
        "    loss = model(batch, labels=batch).loss\n",
        "    log_metrics(step, {'lr': get_lr(), 'samples': step*samples_per_step,\n",
        "                       'steps': completed_steps, 'loss/train': loss.item()})\n",
        "    loss = loss / args.gradient_accumulation_steps\n",
        "    accelerator.backward(loss)\n",
        "    if step % args.gradient_accumulation_steps == 0:\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        completed_steps += 1\n",
        "    if step % args.save_checkpoint_steps == 0:\n",
        "        logger.info('Evaluating and saving model checkpoint')\n",
        "        eval_loss, perplexity = evaluate()\n",
        "        log_metrics(step, {'loss/eval': eval_loss, 'perplexity': perplexity})\n",
        "        accelerator.wait_for_everyone()\n",
        "        unwrapped_model = accelerator.unwrap_model(model)\n",
        "        if accelerator.is_main_process:\n",
        "            unwrapped_model.save_pretrained(\"./\")\n",
        "            hf_repo.push_to_hub(commit_message=f'step {step}')\n",
        "        model.train()\n",
        "    if completed_steps >= args.max_train_steps:\n",
        "        break\n",
        "\n",
        "# Evaluate and save the last checkpoint\n",
        "logger.info('Evaluating and saving model after training')\n",
        "eval_loss, perplexity = evaluate()\n",
        "log_metrics(step, {'loss/eval': eval_loss, 'perplexity': perplexity})\n",
        "accelerator.wait_for_everyone()\n",
        "unwrapped_model = accelerator.unwrap_model(model)\n",
        "if accelerator.is_main_process:\n",
        "    unwrapped_model.save_pretrained(\"./\")\n",
        "    hf_repo.push_to_hub(commit_message=f'final model')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s deconstruct the\n",
        "script a little bit and highlight the most important parts:\n",
        "\n",
        "1. Model saving\n",
        "\n",
        "We run the script from within the model repository, and at the start we check out\n",
        "a new branch named after the run_name we get from Weights & Biases. Later, we\n",
        "commit the model at each checkpoint and push it to the Hub.\n",
        "\n",
        "2. Optimization\n",
        "\n",
        "For the model optimization we use AdamW with a cosine learning rate schedule\n",
        "after a linear warming-up period. For the hyperparameters, we closely follow the\n",
        "parameters described in the GPT-3 paper for similar-sized models.8\n",
        "\n",
        "3. Evaluation\n",
        "\n",
        "We evaluate the model on the evaluation set every time we save—that is, every\n",
        "save_checkpoint_steps and after training. Along with the validation loss we\n",
        "also log the validation perplexity.\n",
        "\n",
        "4. Gradient accumulation and checkpointing\n",
        "\n",
        "The required batch sizes don’t fit in a GPU’s memory, even when we run on the\n",
        "latest GPUs. Therefore, we implement gradient accumulation, which gathers gra‐\n",
        "dients over several backward passes and optimizes once enough gradients are\n",
        "accumulated.\n",
        "\n"
      ],
      "metadata": {
        "id": "GfGE_HvN2_bK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One aspect that might still be a bit obscure is what it means to train a model on mul‐\n",
        "tiple GPUs. There are several approaches to train models in a distributed fashion\n",
        "depending on the size of your model and volume of data. The approach utilized by\n",
        "Accelerate is called DataDistributedParallelism (DDP). The main advantage of\n",
        "this approach is that it allows you to train models faster with larger batch sizes that\n",
        "wouldn’t fit into any single GPU. The process is illustrated in Figure 10-6."
      ],
      "metadata": {
        "id": "d0PTEDuI3naq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBAs8h8R0UTg"
      },
      "source": [
        "<img alt=\"DDP\" caption=\"Illustration of the processing steps in DDP with four GPUs\" src=\"images/chapter10_ddp.png\" id=\"ddp\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s go through the pipeline step by step:\n",
        "1. Each worker consists of a GPU. In Accelerate, there is a dataloader running on\n",
        "the main process that prepares the batches of data and sends them to all the\n",
        "workers.\n",
        "2. Each GPU receives a batch of data and calculates the loss and respective accumu‐\n",
        "lated gradients from forward and backward passes with a local copy of the model.\n",
        "3. The gradients from each node are averaged with a reduce pattern, and the aver‐\n",
        "aged gradients are sent back to each worker.\n",
        "\n",
        "4. The gradients are applied using the optimizer on each node individually.\n",
        "Although this might seem like redundant work, it avoids transferring copies of\n",
        "the large models between nodes. We’ll need to update the model at least once,\n",
        "and without this approach the other nodes would each need to wait until they’d\n",
        "received the updated version.\n",
        "5. Once all models are updated we start all over again, with the main worker pre‐\n",
        "paring new batches."
      ],
      "metadata": {
        "id": "i9CODf2-3t2F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8Jvlu6a0UTg"
      },
      "source": [
        "### The Training Run"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll save the training script in a file called codeparrot_training.py so that we can exe‐\n",
        "cute it on our training server. To make life even easier, we’ll add it along with a\n",
        "requirements.txt file containing all the required Python dependencies to the model\n",
        "repository on the Hub.\n",
        "\n",
        "\n",
        "Running the training script with these settings on that infrastructure takes about 24\n",
        "hours and 7 days for the small and large models, respectively. If you train your own\n",
        "custom model, make sure your code runs smoothly on smaller infrastructure in order\n",
        "to make sure that expensive long run goes smoothly as well."
      ],
      "metadata": {
        "id": "nRv0AMwa32Y_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdLxZjXf0UTg"
      },
      "source": [
        "## Results and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HymgLSSb0UTg",
        "outputId": "db9a4895-f2ee-48d4-ef73-423bca033d15"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-10-20 18:29:01.107727: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2021-10-20 18:29:01.107759: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        }
      ],
      "source": [
        "#hide_output\n",
        "from transformers import pipeline, set_seed\n",
        "\n",
        "model_ckpt = 'transformersbook/codeparrot-small'\n",
        "generation = pipeline('text-generation', model=model_ckpt, device=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use the generation pipeline to generate candidate completions from a\n",
        "given prompt. By default, the pipeline will generate code until a predefined maximum\n",
        "length, and the output could contain multiple functions or classes. So, to keep the\n",
        "outputs concise, we’ll implement a first_block() function that uses regular expres‐\n",
        "sions to extract the first occurrence of a function or class. The complete_code()\n",
        "function below applies this logic to print out the completions generated by\n",
        "CodeParrot:"
      ],
      "metadata": {
        "id": "VxuV0m6T4aYf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-m0MVsMW0UTg"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from transformers import set_seed\n",
        "\n",
        "def first_block(string):\n",
        "    return re.split('\\nclass|\\ndef|\\n#|\\n@|\\nprint|\\nif', string)[0].rstrip()\n",
        "\n",
        "def complete_code(pipe, prompt, max_length=64, num_completions=4, seed=1):\n",
        "    set_seed(seed)\n",
        "    gen_kwargs = {\"temperature\":0.4, \"top_p\":0.95, \"top_k\":0, \"num_beams\":1,\n",
        "                  \"do_sample\":True,}\n",
        "    code_gens = generation(prompt, num_return_sequences=num_completions,\n",
        "                            max_length=max_length, **gen_kwargs)\n",
        "    code_strings = []\n",
        "    for code_gen in code_gens:\n",
        "        generated_code = first_block(code_gen['generated_text'][len(prompt):])\n",
        "        code_strings.append(generated_code)\n",
        "    print(('\\n'+'='*80 + '\\n').join(code_strings))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "Let’s start with a simple example and have the model write a function for us that cal‐\n",
        "culates the area of a rectangle:"
      ],
      "metadata": {
        "id": "eopr4_4X4WG_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a7vIvrZ0UTg",
        "outputId": "c1c3a37e-e0ca-488b-ff70-ddb940785604"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    return math.sqrt(a * b)\n",
            "================================================================================\n",
            "\n",
            "    return a * b / 2.0\n",
            "================================================================================\n",
            "\n",
            "    return a * b\n",
            "================================================================================\n",
            "\n",
            "    return a * b / a\n"
          ]
        }
      ],
      "source": [
        "prompt = '''def area_of_rectangle(a: float, b: float):\n",
        "    \"\"\"Return the area of the rectangle.\"\"\"'''\n",
        "complete_code(generation, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWHTAcy20UTg",
        "outputId": "dd4d5b18-774d-4004-ac2b-e76a4fdf5be1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    if not html:\n",
            "        return []\n",
            "    return [url for url in re.findall(r'<a href=\"(/[^/]+/[^\"]+?)\">', html)]\n",
            "================================================================================\n",
            "\n",
            "    return [url for url in re.findall(r'<a href=\"(.*?)\"', html)\n",
            "            if url]\n",
            "================================================================================\n",
            "\n",
            "    return [url for url in re.findall(r'<a href=\"(/.*)\",', html)]\n",
            "================================================================================\n",
            "\n",
            "    return re.findall(r'<a href=\"(.*?)\" class=\"url\"[^>]*>', html)\n"
          ]
        }
      ],
      "source": [
        "prompt = '''def get_urls_from_html(html):\n",
        "    \"\"\"Get all embedded URLs in a HTML string.\"\"\"'''\n",
        "complete_code(generation, prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although it didn’t quite get it right in the second attempt, the other three generations\n",
        "are correct. We can test the function on the Hugging Face home page:"
      ],
      "metadata": {
        "id": "geJ1Kd8O4ozG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hk1TiJn80UTg",
        "outputId": "c8532cda-3e21-4c63-ab24-2c56dc5a4ad4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://github.com/huggingface/transformers | /allenai | /facebook |\n",
            "/asteroid-team | /google | /amazon | /speechbrain | /microsoft | /grammarly |\n",
            "/models | /inference-api | /distilbert-base-uncased |\n",
            "/dbmdz/bert-large-cased-finetuned-conll03-english |\n",
            "https://huggingface.co/transformers | https://arxiv.org/abs/1811.06031 |\n",
            "https://arxiv.org/abs/1803.10631 | https://transformer.huggingface.co/ | /coref\n",
            "| https://medium.com/huggingface/distilbert-8cf3380435b5\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def get_urls_from_html(html):\n",
        "    return [url for url in re.findall(r'<a href=\"(.*?)\"', html) if url]\n",
        "\n",
        "print(\" | \".join(get_urls_from_html(requests.get('https://hf.co/').text)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that all the URLs starting with https are external pages, whereas the oth‐\n",
        "ers are subpages of the main website. That’s exactly what we wanted. Finally, let’s load\n",
        "the large model and see if we can use it to translate a function from pure Python to\n",
        "NumPy:"
      ],
      "metadata": {
        "id": "G35_dHHr4uR9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdQXSl960UTh"
      },
      "source": [
        "> **NOTE**: In the following code block, a large GPT-2 checkpoint is loaded into memory. On platforms like Colab and Kaggle, this can cause the instance to crash due to insufficient RAM or GPU memory. You can still run the example if you replace the large model with the small one by using `model_ckpt = \"transformersbook/codeparrot-small\"`.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVFoxnqh0UTh",
        "outputId": "e4527065-84f8-492d-e20e-7440e04dc9cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    return np.mean(a)\n",
            "================================================================================\n",
            "\n",
            "    return np.mean(a)\n",
            "================================================================================\n",
            "\n",
            "    return np.mean(a)\n",
            "================================================================================\n",
            "\n",
            "    return np.mean(a)\n"
          ]
        }
      ],
      "source": [
        "model_ckpt = 'transformersbook/codeparrot'\n",
        "generation = pipeline('text-generation', model=model_ckpt, device=0)\n",
        "\n",
        "prompt = '''# a function in native python:\n",
        "def mean(a):\n",
        "    return sum(a)/len(a)\n",
        "\n",
        "# the same function using numpy:\n",
        "import numpy as np\n",
        "def mean(a):'''\n",
        "complete_code(generation, prompt, max_length=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That worked! Let’s see if we can also use the CodeParrot model to help us build a\n",
        "Scikit-learn model:"
      ],
      "metadata": {
        "id": "28YM6nnP4xfB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZSKPIva0UTh",
        "outputId": "1128680b-a30f-4f78-c122-dddf546f04a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "reg = DummyRegressor()\n",
            "\n",
            "forest = RandomForestClassifier(n_estimators=20)\n",
            "\n",
            "forest.fit(X, y)\n",
            "================================================================================\n",
            "\n",
            "clf = ExtraTreesClassifier(n_estimators=100, max_features='sqrt')\n",
            "clf.fit(X, y)\n",
            "================================================================================\n",
            "\n",
            "clf = RandomForestClassifier(n_estimators=20, n_jobs=n_jobs, random_state=1)\n",
            "clf.fit(X, y)\n",
            "================================================================================\n",
            "\n",
            "clf = RandomForestClassifier(n_estimators=20)\n",
            "clf.fit(X, y)\n"
          ]
        }
      ],
      "source": [
        "prompt = '''X = np.random.randn(100, 100)\n",
        "y = np.random.randint(0, 1, 100)\n",
        "\n",
        "# fit random forest classifier with 20 estimators'''\n",
        "complete_code(generation, prompt, max_length=96)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Chapter 5 we explored a few metrics to measure the quality of generated text.\n",
        "Among these was the BLEU score, which is frequently used for that purpose. While\n",
        "this metric has limitations in general, it is particularly badly suited for our use case.\n",
        "The BLEU score measures the overlap of n-grams between the reference texts and the\n",
        "generated texts. When writing code we have a lot of freedom in terms of variables and classes, and the success of a program does not depend on the naming scheme as\n",
        "long as it is consistent. However, the BLEU score would punish a generation that\n",
        "deviates from the reference naming, which might in fact be almost impossible to pre‐\n",
        "dict (even for a human coder).\n",
        "In software development there are much better and more reliable ways to measure\n",
        "the quality of code, such as unit tests. This is how all the OpenAI Codex models were\n",
        "evaluated: by running several code generations for coding tasks through a set of unit\n",
        "tests and calculating the fraction of generations that pass the tests.10"
      ],
      "metadata": {
        "id": "WsqM3fH448Bs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07oa1RAk0UTh"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvJr59-M0UTh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}